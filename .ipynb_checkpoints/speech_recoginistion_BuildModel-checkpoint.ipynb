{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:#ffc0cb;font-size:70px;font-family:Georgia;text-align:center;\"><strong>Speech Emotion Recognition</strong></h1>\n",
    "\n",
    "### <b>Author Name: Nguyen Dang Huynh Chau</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:#ffc0cb;font-size:40px;font-family:Georgia;text-align:center;\"><strong> üìú Table of Content</strong></h1>\n",
    "\n",
    "### 1. [Data Preparation](#1)\n",
    "\n",
    "1.1 [Introduction](#1.1) \n",
    "\n",
    "1.2 [Importing Necessary Libraries and datasets](#1.2)\n",
    "\n",
    "1.3 [Data Retrieving](#1.3)\n",
    "\n",
    "1.3.1 [SAVEE Dataset](#1.3.1)\n",
    "1.3.2 [SAVEE Dataset](#1.3.2)\n",
    "1.3.3 [SAVEE Dataset](#1.3.3)\n",
    "1.3.4 [SAVEE Dataset](#1.3.4)\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "### 2. [Feature Extract](#2)\n",
    "\n",
    "2.1 [About This Dataset](#2.1)\n",
    "\n",
    "2.2 [Data preprocessing](#2.2)\n",
    "\n",
    "> - 2.2.1 [Drop column `ID` and `Insurance`](#2.2.1)\n",
    "> - 2.2.2 [Rename column `Sepssis`](#2.2.2)  \n",
    "> - 2.2.3 [Convert `Sepsis` in to binary number](#2.2.3)\n",
    "> - 2.2.4 [Drop Duplicate](#2.2.4)  \n",
    "> - 2.2.5 [Convert Data Type](#2.2.5)  \n",
    "\n",
    "2.3 [Drop column](#2.3)\n",
    "\n",
    "> - 2.3.1 [Check correllation for dropping](#2.3.1)\n",
    "> - 2.3.2 [Check missing values for dropping](#2.3.2)  \n",
    "\n",
    "2.4 [Upper Case the content](#2.4)\n",
    "\n",
    "2.5 [Extra-whitespaces](#2.5)\n",
    "\n",
    "2.6 [Descriptive statistics for Central Tendency](#2.6)\n",
    "\n",
    "> - 2.6.1 [Overview statistics](#2.6.1)\n",
    "> - 2.6.2 [Domain Knowledge](#2.6.2)  \n",
    "> - 2.6.3 [Detect Outliers](#2.6.2)  \n",
    "\n",
    "2.7 [Save The Intermediate Data](#2.8)\n",
    "\n",
    "<br>\n",
    "\n",
    "### 3. [Data exploration (EDA)](#3)\n",
    "\n",
    "3.1 [Overall look on target variable](#3.1)\n",
    "\n",
    "> - 3.1.1 [Distribution of Sepsis](#3.1.1) \n",
    "> - 3.1.2 [Proportion of Sepsis](#3.1.1) \n",
    "\n",
    "3.2 [Frequency of each corresponiding Target variable type](#3.2)\n",
    "\n",
    "> - 3.2.1 [How old are they?](#3.2.1) \n",
    "> - 3.2.2 [How much they weight?](#3.2.2) \n",
    "> - 3.2.3 [How high PL (Blood Work Result-1 (mu U/ml)) that the Sepsis is likely to get?](#3.2.3) \n",
    "> - 3.2.4 [How high PR ((Blood Pressure (mm Hg)) that the Sepsis is likely to get?](#3.2.4) \n",
    "> - 3.2.5 [How high SK (Blood Work Result-2 (mm) that the Sepsis is likely to get?](#3.2.5) \n",
    "> - 3.2.6 [How high TS (Blood Work Result-3 (mu U/ml)) that the Sepsis is likely to get?](#3.2.6) \n",
    "> - 3.2.7 [How high BD2 (Blood Work Result-4 (mu U/ml)) that the Sepsis is likely to get?](#3.2.7) \n",
    "> - 3.2.8 [How high BD2 (Blood Work Result-4 (mu U/ml)) that the Sepsis is likely to get?](#3.2.8) \n",
    "> - 3.2.9 [Scatter matrix](#3.2.8) \n",
    "\n",
    "3.3 [Statistical Test for Correlation](#3.3)\n",
    "\n",
    "3.4 [Summary](#3.3)\n",
    "\n",
    "<br>\n",
    "\n",
    "### 4. [Feature Engineering](#4)\n",
    "\n",
    "4.1 [Class Imbalancing](#4.1)\n",
    "\n",
    "4.2 [Splitting the training data](#4.2)\n",
    "\n",
    "4.3 [Feature Scaling](#4.3)\n",
    "\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "### 5. [Model Building](#5) \n",
    "\n",
    "5.1 [Logistic Regression](#5.1)\n",
    "\n",
    "> - 5.1.1 [Train Model](#5.1.1)\n",
    "> - 5.1.2 [Model Evaluation](#5.1.2)\n",
    "> - 5.1.3 [Hypertuning parameter](#5.1.3)\n",
    "> - 5.1.4 [Retrain](#5.1.4)\n",
    "> - 5.1.5 [Conclusion](#5.1.5)\n",
    "\n",
    "5.2 [Decision Tree](#5.2)\n",
    "\n",
    "> - 5.2.1 [Train Model](#5.2.1)\n",
    "> - 5.2.2 [Hypertuning & Pruning](#5.2.2)\n",
    "\n",
    "> - 5.2.2.a [Post-Pruning](#5.2.2.a)\n",
    "> - 5.2.2.b [Pre-Pruning](#5.2.2.b)\n",
    "> - 5.2.2.c [Hypertuning parameter](#5.2.2.c)\n",
    "\n",
    "> - 5.2.3 [Hypertuning parameter](#5.2.3)\n",
    "> - 5.2.4 [Conclusion](#5.2.3)\n",
    "\n",
    "5.3 [Random Forest](#5.3)\n",
    "\n",
    "> - 5.3.1 [Train Model](#5.3.1)\n",
    "> - 5.3.2 [Model Evaluation](#5.3.2)\n",
    "> - 5.3.3 [Hypertuning parameter](#5.3.3)\n",
    "> - 5.3.4 [Retrain](#5.3.4)\n",
    "> - 5.3.5 [Conclusion](#5.3.5)\n",
    "\n",
    "<br>\n",
    "\n",
    "### 6. [Conculsions](#5)\n",
    "\n",
    "<br>\n",
    "\n",
    "### 7. [References](#7)\n",
    "\n",
    "<br>\n",
    "\n",
    "### 8. [Appendix](#8)\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a>\n",
    "<h1 style=\"color:#ffc0cb;font-size:40px;font-family:Georgia;text-align:center;\"><strong> ‚úçÔ∏è 1. Data Preparation</strong></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1.1\"></a>\n",
    "# Ô∏èüéØ 1.1 Introduction\n",
    "****\n",
    "\n",
    "<p style=\"list-style-type:circle;font-family:Yu Mincho Demibold;font-size:20px;color:black\"> Speech Emotion Recognition, or SER for short, is the process of trying to identify affective and emotional states in speech. This makes use of the fact that tone and pitch in the voice frequently convey underlying emotion. In order to comprehend human emotion, animals like dogs and horses also use this phenomenon. The component of speech recognition that is growing in popularity and demand is emotion recognition. Although there are ways to identify emotions from data using machine learning methods, this research tries to use deep learning to do so. SER (Speech Emotion Recognition) is used in contact centers to categorize conversations based on emotions and can be used as a performance metric for conversational analysis to identify the dissatisfied client, customer satisfaction, and other factors that can help businesses improve their services. It can also be utilized as an in-car board system based on information about the driver's mental state to ensure the driver's safety and prevent accidents.</p>\n",
    "\n",
    "## üì£ What datasets are used in this project?\n",
    "\n",
    "<ul style=\"list-style-type:circle;font-family:Yu Mincho Demibold;font-size:20px;color:black\">\n",
    "    <li> Crowd-sourced Emotional Mutimodal Actors Dataset (Crema-D) </li>\n",
    "    <li> Ryerson Audio-Visual Database of Emotional Speech and Song (Ravdess) </li>\n",
    "    <li> Surrey Audio-Visual Expressed Emotion (Savee) </li>\n",
    "    <li> Toronto emotional speech set (Tess) </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1.2\"></a>\n",
    "# ‚ú¥Ô∏è 1.2 Importing Necessary Libraries and datasets\n",
    "****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-09-23T10:07:11.273954Z",
     "iopub.status.busy": "2022-09-23T10:07:11.273593Z",
     "iopub.status.idle": "2022-09-23T10:07:16.072554Z",
     "shell.execute_reply": "2022-09-23T10:07:16.071414Z",
     "shell.execute_reply.started": "2022-09-23T10:07:11.273893Z"
    }
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'pad_sequences' from 'keras.preprocessing.sequence' (/opt/miniconda3/envs/tensorflow/lib/python3.9/site-packages/keras/preprocessing/sequence.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sequence\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Tokenizer\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msequence\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pad_sequences\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sequential, Model, model_from_json\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dense, Embedding, LSTM\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'pad_sequences' from 'keras.preprocessing.sequence' (/opt/miniconda3/envs/tensorflow/lib/python3.9/site-packages/keras/preprocessing/sequence.py)"
     ]
    }
   ],
   "source": [
    "# Importing required libraries \n",
    "# Keras\n",
    "import keras\n",
    "from keras import regularizers\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential, Model, model_from_json\n",
    "from keras.layers import Dense, Embedding, LSTM\n",
    "from keras.layers import Input, Flatten, Dropout, Activation, BatchNormalization\n",
    "from keras.layers import Conv1D, MaxPooling1D, AveragePooling1D\n",
    "from keras.utils import np_utils, to_categorical\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# sklearn\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Other  \n",
    "import librosa\n",
    "import librosa.display\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from matplotlib.pyplot import specgram\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import glob \n",
    "import os\n",
    "import pickle\n",
    "import IPython.display as ipd  # To play sound in the notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"data\"></a>\n",
    "## 1. Data preparation and processing\n",
    "We saw in [Part 1](https://www.kaggle.com/ejlok1/audio-emotion-recognition-part-1-explore-data) and [Part 2](https://www.kaggle.com/ejlok1/audio-emotion-recognition-part-2-feature-extra) the way we process the audio file into data and the MFCC features we extracted. We're going to do the same thing here except we process the entirity of the audio files. First up we need the reference file that contains the path to the raw audio files for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets pick up the meta-data that we got from our first part of the Kernel\n",
    "ref = pd.read_csv(\"/kaggle/input/data-path/Data_path.csv\")\n",
    "ref.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note this takes a couple of minutes (~10 mins) as we're iterating over 4 datasets \n",
    "df = pd.DataFrame(columns=['feature'])\n",
    "\n",
    "# loop feature extraction over the entire dataset\n",
    "counter=0\n",
    "for index,path in enumerate(ref.path):\n",
    "    X, sample_rate = librosa.load(path\n",
    "                                  , res_type='kaiser_fast'\n",
    "                                  ,duration=2.5\n",
    "                                  ,sr=44100\n",
    "                                  ,offset=0.5\n",
    "                                 )\n",
    "    sample_rate = np.array(sample_rate)\n",
    "    \n",
    "    # mean as the feature. Could do min and max etc as well. \n",
    "    mfccs = np.mean(librosa.feature.mfcc(y=X, \n",
    "                                        sr=sample_rate, \n",
    "                                        n_mfcc=13),\n",
    "                    axis=0)\n",
    "    df.loc[counter] = [mfccs]\n",
    "    counter=counter+1   \n",
    "\n",
    "# Check a few records to make sure its processed successfully\n",
    "print(len(df))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
