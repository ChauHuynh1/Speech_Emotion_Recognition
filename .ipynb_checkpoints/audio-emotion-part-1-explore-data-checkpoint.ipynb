{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Audio Emotion classifier</center>\n",
    "## <center>Part 1 - Data Exploration</center>\n",
    "#### <center> 19th August 2019 </center> \n",
    "#####  <center> Eu Jin Lok </center> \n",
    "\n",
    "\n",
    "\n",
    "# Introduction \n",
    "We are going to build an audio emotion classifier. But why you ask, are we doing this? Well, for a few reasons: \n",
    "\n",
    "- **Its becoming fairly important**<br/>\n",
    "At least, here in Australia, due to the recent fallout of the Royal Commision, there's more scrutiny than ever into the financial sector. One of the areas that traditionaly have been ingored by the government is contact centres, many of these generate contracts through telephone conversations. So compliance is a hot topic right now \n",
    "\n",
    "- **Because we can**<br/>\n",
    "With recent advancements of Deep Learning, better hardware and more open sourcing of data, this enables us to build the capability that we couldn't before. So, why not.  \n",
    "\n",
    "- **Accessibility**<br/>\n",
    "I've specifically chosen emotion as our target because its one of the more accesible **labeled** dataset. Don't misunderstand, there are many good quality audio datasets out there, but many are either not relevant (eg. background noises), or locked behind paid wall. Emotions are probably relevant enough since we are dealing with conversations between agent and customer, and there's variety of sources. Also, emotions are general enough (not context dependent), so we can apply to a vast number of different projects\n",
    "\n",
    "This is going to be a 6 to 7 part series at least. Could be more, but not less. Part 2 covers feature extraction whilst Part 3 and 4 we will dive into the modelling. But to train a model, we need data. So Part 1 here, we are going to check out a few data sources which are all open sourced:\n",
    "\n",
    "- Surrey Audio-Visual Expressed Emotion [(SAVEE)](https://www.kaggle.com/ejlok1/surrey-audiovisual-expressed-emotion-savee)\n",
    "- Ryerson Audio-Visual Database of Emotional Speech and Song [(RAVDESS)](https://www.kaggle.com/uwrfkaggler/ravdess-emotional-speech-audio)\n",
    "- Toronto emotional speech set [(TESS)](https://www.kaggle.com/ejlok1/toronto-emotional-speech-set-tess)\n",
    "- Crowd-sourced Emotional Mutimodal Actors Dataset [(CREMA-D)](https://www.kaggle.com/ejlok1/cremad)\n",
    "\n",
    "The ultimate end game is to get the best accuracy that will generalise across unseen data. Right now its unclear how we mix and match (or excluded) our datasets so that's why we need to explore it first. So lets begin\n",
    "\n",
    "### Contents\n",
    "1. [SAVEE dataset](#savee)\n",
    "    * [Load the dataset](#savee_load)\n",
    "    * [Explore the data](#savee_explore)\n",
    "    * [Conclusion](#savee_con)\n",
    "2. [RAVDESS dataset](#ravdess)\n",
    "    * [Load the dataset](#ravdess_load)\n",
    "    * [Explore the data](#ravdess_explore)\n",
    "    * [Conclusion](#ravdess_con)\n",
    "3. [TESS dataset](#tess)\n",
    "    * [Load the dataset](#tess_load)\n",
    "    * [Explore the data](#tess_explore)\n",
    "    * [Conclusion](#tess_con)\n",
    "4. [CREMA-D dataset](#crema)\n",
    "    * [Load the dataset](#crema_load)\n",
    "    * [Explore the data](#crema_explore)\n",
    "    * [Conclusion](#crema_con)\n",
    "5. [Final thoughts](#final)\n",
    "\n",
    "Note that the other parts are now available:\n",
    "* [Part 2 | Feature Extract](https://www.kaggle.com/ejlok1/audio-emotion-part-2-feature-extract)\n",
    "* [Part 3 | Baseline model](https://www.kaggle.com/ejlok1/audio-emotion-part-3-baseline-model)\n",
    "* [Part 4 | Apply to new audio data](https://www.kaggle.com/ejlok1/audio-emotion-part-4-apply-to-new-audio-data)\n",
    "* [Part 5 | Data augmentation](https://www.kaggle.com/ejlok1/audio-emotion-part-5-data-augmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-09-23T10:07:11.273954Z",
     "iopub.status.busy": "2022-09-23T10:07:11.273593Z",
     "iopub.status.idle": "2022-09-23T10:07:16.072554Z",
     "shell.execute_reply": "2022-09-23T10:07:16.071414Z",
     "shell.execute_reply.started": "2022-09-23T10:07:11.273893Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import libraries \n",
    "import librosa\n",
    "import librosa.display\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from matplotlib.pyplot import specgram\n",
    "import pandas as pd\n",
    "import glob \n",
    "from sklearn.metrics import confusion_matrix\n",
    "import IPython.display as ipd  # To play sound in the notebook\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "# ignore warnings \n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 4 sources of the datasets are all on Kaggle so I've just imported them into the workspace. The directory path to the 4 sources in this environment are below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-23T10:07:16.075677Z",
     "iopub.status.busy": "2022-09-23T10:07:16.075320Z",
     "iopub.status.idle": "2022-09-23T10:07:16.304437Z",
     "shell.execute_reply": "2022-09-23T10:07:16.303624Z",
     "shell.execute_reply.started": "2022-09-23T10:07:16.075608Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['JK_sa01.wav', 'JK_sa15.wav', 'DC_n13.wav', 'DC_su09.wav', 'DC_n07.wav']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "#    for filename in filenames:\n",
    "#        print(os.path.join(dirname, filename))\n",
    "\n",
    "Ravdess = \"ravdess/\"\n",
    "Crema = \"cremad\"\n",
    "Tess = \"tess\"\n",
    "Savee = \"savee\"\n",
    "\n",
    "\n",
    "# Run one example \n",
    "dir_list = os.listdir(Savee)\n",
    "dir_list[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"savee\"></a>\n",
    "##  <center> 1. SAVEE dataset <center>\n",
    "The audio files are named in such a way that the prefix letters describes the emotion classes as follows:\n",
    "- 'a' = 'anger'\n",
    "- 'd' = 'disgust'\n",
    "- 'f' = 'fear'\n",
    "- 'h' = 'happiness'\n",
    "- 'n' = 'neutral'\n",
    "- 'sa' = 'sadness'\n",
    "- 'su' = 'surprise' \n",
    "\n",
    "The original source has 4 folders each representing a speaker, but i've bundled all of them into one single folder and thus the first 2 letter prefix of the filename represents the speaker initials. Eg. 'DC_d03.wav' is the 3rd disgust sentence uttered by the speaker DC. It's  worth nothing that they are all male speakers only. This isn't an issue as we'll balance it out with the TESS dataset which is just female only. So lets check out the distribution of the emotions..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"savee_load\"></a>\n",
    "###  Load the dataset \n",
    "I'm not going to be reading the entire audio to memory. Rather I'm just going to read the meta-data associated with it. Cause at this point I just want a high level snapshot of some statistics. And then I might just load 1 or 2 audio files and expand on it. \n",
    "\n",
    "So lets take 2 different emotions and play it just to get a feel for what we are dealing with. Ie. whether the data (audio) quality is good. It gives us an early insight as to how likely our classifier is going to be successful.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-23T10:07:16.305821Z",
     "iopub.status.busy": "2022-09-23T10:07:16.305546Z",
     "iopub.status.idle": "2022-09-23T10:07:16.333260Z",
     "shell.execute_reply": "2022-09-23T10:07:16.332266Z",
     "shell.execute_reply.started": "2022-09-23T10:07:16.305758Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "male_neutral     120\n",
       "male_sad          60\n",
       "male_surprise     60\n",
       "male_fear         60\n",
       "male_disgust      60\n",
       "male_happy        60\n",
       "male_angry        60\n",
       "Name: labels, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the data location for SAVEE\n",
    "dir_list = os.listdir(Savee)\n",
    "\n",
    "# parse the filename to get the emotions\n",
    "emotion=[]\n",
    "path = []\n",
    "for i in dir_list:\n",
    "    if i[-8:-6]=='_a':\n",
    "        emotion.append('male_angry')\n",
    "    elif i[-8:-6]=='_d':\n",
    "        emotion.append('male_disgust')\n",
    "    elif i[-8:-6]=='_f':\n",
    "        emotion.append('male_fear')\n",
    "    elif i[-8:-6]=='_h':\n",
    "        emotion.append('male_happy')\n",
    "    elif i[-8:-6]=='_n':\n",
    "        emotion.append('male_neutral')\n",
    "    elif i[-8:-6]=='sa':\n",
    "        emotion.append('male_sad')\n",
    "    elif i[-8:-6]=='su':\n",
    "        emotion.append('male_surprise')\n",
    "    else:\n",
    "        emotion.append('male_error') \n",
    "    path.append(Savee + i)\n",
    "    \n",
    "# Now check out the label count distribution \n",
    "SAVEE_df = pd.DataFrame(emotion, columns = ['labels'])\n",
    "SAVEE_df['source'] = 'SAVEE'\n",
    "SAVEE_df = pd.concat([SAVEE_df, pd.DataFrame(path, columns = ['path'])], axis = 1)\n",
    "SAVEE_df.labels.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"savee_explore\"></a>\n",
    "### Explore the data\n",
    "So a quick introduction about the audio data before we carry on too far. A sound is a vibration of air molecules, and our eardrums interprets it as sound, or music. The below wave plot is a graphical representation of a sound wave vibration overtime. Its in this wave that we need to find the key pattern that will help us distinguish the different emotions. \n",
    "\n",
    "There's a nice website that gives a very nice summary of the core audio concepts [here](http://help.nchsoftware.com/help/en/wavepad/win/concepts.html). We're going to plot one or two audio files here selected randomly, just to get a feel for the type of data we're dealing with. Eg. Does it contain lots of background noise? Is the emotions clear? etc. The idea being that, if a human struggles to interpret the data, then its very likely the model isn't going to do a very good job either. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-23T10:07:16.335691Z",
     "iopub.status.busy": "2022-09-23T10:07:16.334995Z",
     "iopub.status.idle": "2022-09-23T10:07:17.597960Z",
     "shell.execute_reply": "2022-09-23T10:07:17.597040Z",
     "shell.execute_reply.started": "2022-09-23T10:07:16.335359Z"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'saveeDC_f11.wav'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/envs/tf/lib/python3.10/site-packages/librosa/core/audio.py:164\u001b[0m, in \u001b[0;36mload\u001b[0;34m(path, sr, mono, offset, duration, dtype, res_type)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 164\u001b[0m     y, sr_native \u001b[38;5;241m=\u001b[39m \u001b[43m__soundfile_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mduration\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;66;03m# If soundfile failed, try audioread instead\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tf/lib/python3.10/site-packages/librosa/core/audio.py:195\u001b[0m, in \u001b[0;36m__soundfile_load\u001b[0;34m(path, offset, duration, dtype)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;66;03m# Otherwise, create the soundfile object\u001b[39;00m\n\u001b[0;32m--> 195\u001b[0m     context \u001b[38;5;241m=\u001b[39m \u001b[43msf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSoundFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context \u001b[38;5;28;01mas\u001b[39;00m sf_desc:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tf/lib/python3.10/site-packages/soundfile.py:629\u001b[0m, in \u001b[0;36mSoundFile.__init__\u001b[0;34m(self, file, mode, samplerate, channels, subtype, endian, format, closefd)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info \u001b[38;5;241m=\u001b[39m _create_info_struct(file, mode, samplerate, channels,\n\u001b[1;32m    628\u001b[0m                                  \u001b[38;5;28mformat\u001b[39m, subtype, endian)\n\u001b[0;32m--> 629\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode_int\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosefd\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mset\u001b[39m(mode)\u001b[38;5;241m.\u001b[39missuperset(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr+\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseekable():\n\u001b[1;32m    631\u001b[0m     \u001b[38;5;66;03m# Move write position to 0 (like in Python file objects)\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tf/lib/python3.10/site-packages/soundfile.py:1183\u001b[0m, in \u001b[0;36mSoundFile._open\u001b[0;34m(self, file, mode_int, closefd)\u001b[0m\n\u001b[1;32m   1182\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid file: \u001b[39m\u001b[38;5;132;01m{0!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname))\n\u001b[0;32m-> 1183\u001b[0m \u001b[43m_error_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_snd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msf_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_ptr\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1184\u001b[0m \u001b[43m             \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mError opening \u001b[39;49m\u001b[38;5;132;43;01m{0!r}\u001b[39;49;00m\u001b[38;5;124;43m: \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode_int \u001b[38;5;241m==\u001b[39m _snd\u001b[38;5;241m.\u001b[39mSFM_WRITE:\n\u001b[1;32m   1186\u001b[0m     \u001b[38;5;66;03m# Due to a bug in libsndfile version <= 1.0.25, frames != 0\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m     \u001b[38;5;66;03m# when opening a named pipe in SFM_WRITE mode.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m     \u001b[38;5;66;03m# See http://github.com/erikd/libsndfile/issues/77.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tf/lib/python3.10/site-packages/soundfile.py:1357\u001b[0m, in \u001b[0;36m_error_check\u001b[0;34m(err, prefix)\u001b[0m\n\u001b[1;32m   1356\u001b[0m err_str \u001b[38;5;241m=\u001b[39m _snd\u001b[38;5;241m.\u001b[39msf_error_number(err)\n\u001b[0;32m-> 1357\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(prefix \u001b[38;5;241m+\u001b[39m _ffi\u001b[38;5;241m.\u001b[39mstring(err_str)\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreplace\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error opening 'saveeDC_f11.wav': System error.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# use the well known Librosa library for this task \u001b[39;00m\n\u001b[1;32m      2\u001b[0m fname \u001b[38;5;241m=\u001b[39m Savee \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDC_f11.wav\u001b[39m\u001b[38;5;124m'\u001b[39m  \n\u001b[0;32m----> 3\u001b[0m data, sampling_rate \u001b[38;5;241m=\u001b[39m \u001b[43mlibrosa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m15\u001b[39m, \u001b[38;5;241m5\u001b[39m))\n\u001b[1;32m      5\u001b[0m librosa\u001b[38;5;241m.\u001b[39mdisplay\u001b[38;5;241m.\u001b[39mwaveplot(data, sr\u001b[38;5;241m=\u001b[39msampling_rate)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tf/lib/python3.10/site-packages/librosa/util/decorators.py:88\u001b[0m, in \u001b[0;36mdeprecate_positional_args.<locals>._inner_deprecate_positional_args.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m extra_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(all_args)\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m extra_args \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 88\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;66;03m# extra_args > 0\u001b[39;00m\n\u001b[1;32m     91\u001b[0m args_msg \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(name, arg)\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m name, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(kwonly_args[:extra_args], args[\u001b[38;5;241m-\u001b[39mextra_args:])\n\u001b[1;32m     94\u001b[0m ]\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tf/lib/python3.10/site-packages/librosa/core/audio.py:170\u001b[0m, in \u001b[0;36mload\u001b[0;34m(path, sr, mono, offset, duration, dtype, res_type)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, (\u001b[38;5;28mstr\u001b[39m, pathlib\u001b[38;5;241m.\u001b[39mPurePath)):\n\u001b[1;32m    169\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPySoundFile failed. Trying audioread instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 170\u001b[0m     y, sr_native \u001b[38;5;241m=\u001b[39m \u001b[43m__audioread_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mduration\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    172\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tf/lib/python3.10/site-packages/librosa/core/audio.py:226\u001b[0m, in \u001b[0;36m__audioread_load\u001b[0;34m(path, offset, duration, dtype)\u001b[0m\n\u001b[1;32m    223\u001b[0m     reader \u001b[38;5;241m=\u001b[39m path\n\u001b[1;32m    224\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    225\u001b[0m     \u001b[38;5;66;03m# If the input was not an audioread object, try to open it\u001b[39;00m\n\u001b[0;32m--> 226\u001b[0m     reader \u001b[38;5;241m=\u001b[39m \u001b[43maudioread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maudio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m reader \u001b[38;5;28;01mas\u001b[39;00m input_file:\n\u001b[1;32m    229\u001b[0m     sr_native \u001b[38;5;241m=\u001b[39m input_file\u001b[38;5;241m.\u001b[39msamplerate\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tf/lib/python3.10/site-packages/audioread/__init__.py:127\u001b[0m, in \u001b[0;36maudio_open\u001b[0;34m(path, backends)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m BackendClass \u001b[38;5;129;01min\u001b[39;00m backends:\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 127\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mBackendClass\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m DecodeError:\n\u001b[1;32m    129\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tf/lib/python3.10/site-packages/audioread/rawread.py:59\u001b[0m, in \u001b[0;36mRawAudioFile.__init__\u001b[0;34m(self, filename)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, filename):\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fh \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     62\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_file \u001b[38;5;241m=\u001b[39m aifc\u001b[38;5;241m.\u001b[39mopen(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fh)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'saveeDC_f11.wav'"
     ]
    }
   ],
   "source": [
    "# use the well known Librosa library for this task \n",
    "fname = Savee + 'DC_f11.wav'  \n",
    "data, sampling_rate = librosa.load(fname)\n",
    "plt.figure(figsize=(15, 5))\n",
    "librosa.display.waveplot(data, sr=sampling_rate)\n",
    "\n",
    "# Lets play the audio \n",
    "ipd.Audio(fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So that's a fearful sample. Not alot of background noise and the speech is very clear. So that's good news for us in terms of data quality. The wave plot doesn't really tell much other than there's a variation in the wave, which is good. If it was just a constant oscilating wave form, like those heart-rate-monitors ... then we definitely have a problem. \n",
    "\n",
    "But we don't which is great. And we can also tell that the audio file is 3 secs. That's good. Its long enough for someone to express an emotion in a sentence, and not too long such that the emotions changes to neutral. How about we play a happy audio file and see how different they are? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-23T10:07:17.601243Z",
     "iopub.status.busy": "2022-09-23T10:07:17.600900Z",
     "iopub.status.idle": "2022-09-23T10:07:18.223222Z",
     "shell.execute_reply": "2022-09-23T10:07:18.222253Z",
     "shell.execute_reply.started": "2022-09-23T10:07:17.601180Z"
    }
   },
   "outputs": [],
   "source": [
    "# Lets play a happy track\n",
    "fname = Savee + 'DC_h11.wav'  \n",
    "data, sampling_rate = librosa.load(fname)\n",
    "plt.figure(figsize=(15, 5))\n",
    "librosa.display.waveplot(data, sr=sampling_rate)\n",
    "\n",
    "# Lets play the audio \n",
    "ipd.Audio(fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"savee_con\"></a>\n",
    "### Conclusion\n",
    "So again, good quality audio. And I can see that the wave form is distinctively different from the fear one. So that's good for our model. I did notice that there's a very short silence period between start and end. We could potentially trim it later to enhance the quality. Also, the sentence uttered is different so its not exactly an apple to apple comparison but it still gives us a good early indication of what we're dealing with. \n",
    "\n",
    "All in all, I'm happy with this data. We're going to use it for our audio emotion classifier. Now lets look at our next audio dataset..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ravdess\"></a>\n",
    "## <center>2. RAVDESS dataset</center>\n",
    "\n",
    "RAVDESS is one of the more common dataset used for this excercise by others. It's well liked because of its quality of speakers, recording and it has 24 actors of different genders. And there's more! You can get it in song format as well. There's something for everyone and their research project. So for convenience, here's the filename identifiers as per the official RAVDESS website:\n",
    "\n",
    "- Modality (01 = full-AV, 02 = video-only, 03 = audio-only).\n",
    "- Vocal channel (01 = speech, 02 = song).\n",
    "- Emotion (01 = neutral, 02 = calm, 03 = happy, 04 = sad, 05 = angry, 06 = fearful, 07 = disgust, 08 = surprised).\n",
    "- Emotional intensity (01 = normal, 02 = strong). NOTE: There is no strong intensity for the 'neutral' emotion.\n",
    "- Statement (01 = \"Kids are talking by the door\", 02 = \"Dogs are sitting by the door\").\n",
    "- Repetition (01 = 1st repetition, 02 = 2nd repetition).\n",
    "- Actor (01 to 24. Odd numbered actors are male, even numbered actors are female).\n",
    "\n",
    "So, here's an example of an audio filename. \n",
    "_02-01-06-01-02-01-12.mp4_\n",
    "\n",
    "This means the meta data for the audio file is:\n",
    "- Video-only (02)\n",
    "- Speech (01)\n",
    "- Fearful (06)\n",
    "- Normal intensity (01)\n",
    "- Statement \"dogs\" (02)\n",
    "- 1st Repetition (01)\n",
    "- 12th Actor (12) - Female (as the actor ID number is even)\n",
    "\n",
    "At my early beginings embarking on this journey, I learnt through the hard way that male and female speakers have to be trained seperately or the model will struggle to get a good accuracy. From reading a few blogs and articles, it seems female has a higher pitch that male. So if we don't tag the gender label to the audio file, it won't be able to detect anger or fear if it was a male speaker. It will just get bucketed into neutral \n",
    "\n",
    "Lets specifically model the 2 speakers seperately. Note that there's a 'calm' emotion and a 'neutral' emotion as seperate. I don't really know the difference but for now, I'll just combined them into the same category."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ravdess_load\"></a>\n",
    "###  Load the dataset \n",
    "Because of the way the entire data was packaged for us, and the format of the audio filename, there's a few more parsing steps required for the RAVDESS dataset compared to SAVEE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-23T10:07:18.225922Z",
     "iopub.status.busy": "2022-09-23T10:07:18.225473Z",
     "iopub.status.idle": "2022-09-23T10:07:18.875381Z",
     "shell.execute_reply": "2022-09-23T10:07:18.874379Z",
     "shell.execute_reply.started": "2022-09-23T10:07:18.225843Z"
    }
   },
   "outputs": [],
   "source": [
    "dir_list = os.listdir(RAV)\n",
    "dir_list.sort()\n",
    "\n",
    "emotion = []\n",
    "gender = []\n",
    "path = []\n",
    "for i in dir_list:\n",
    "    fname = os.listdir(RAV + i)\n",
    "    for f in fname:\n",
    "        part = f.split('.')[0].split('-')\n",
    "        emotion.append(int(part[2]))\n",
    "        temp = int(part[6])\n",
    "        if temp%2 == 0:\n",
    "            temp = \"female\"\n",
    "        else:\n",
    "            temp = \"male\"\n",
    "        gender.append(temp)\n",
    "        path.append(RAV + i + '/' + f)\n",
    "\n",
    "        \n",
    "RAV_df = pd.DataFrame(emotion)\n",
    "RAV_df = RAV_df.replace({1:'neutral', 2:'neutral', 3:'happy', 4:'sad', 5:'angry', 6:'fear', 7:'disgust', 8:'surprise'})\n",
    "RAV_df = pd.concat([pd.DataFrame(gender),RAV_df],axis=1)\n",
    "RAV_df.columns = ['gender','emotion']\n",
    "RAV_df['labels'] =RAV_df.gender + '_' + RAV_df.emotion\n",
    "RAV_df['source'] = 'RAVDESS'  \n",
    "RAV_df = pd.concat([RAV_df,pd.DataFrame(path, columns = ['path'])],axis=1)\n",
    "RAV_df = RAV_df.drop(['gender', 'emotion'], axis=1)\n",
    "RAV_df.labels.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ravdess_explore\"></a>\n",
    "### Explore the data\n",
    "Lets do the same thing again, take 2 audio files, play it and plot it to see what we're dealing with. And how different they are to SAVEE as we go along. Lets start with a fearful track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-23T10:07:18.877017Z",
     "iopub.status.busy": "2022-09-23T10:07:18.876541Z",
     "iopub.status.idle": "2022-09-23T10:07:19.313547Z",
     "shell.execute_reply": "2022-09-23T10:07:19.312633Z",
     "shell.execute_reply.started": "2022-09-23T10:07:18.876967Z"
    }
   },
   "outputs": [],
   "source": [
    "# Pick a fearful track\n",
    "fname = RAV + 'Actor_14/03-01-06-02-02-02-14.wav'  \n",
    "data, sampling_rate = librosa.load(fname)\n",
    "plt.figure(figsize=(15, 5))\n",
    "librosa.display.waveplot(data, sr=sampling_rate)\n",
    "\n",
    "# Lets play the audio \n",
    "ipd.Audio(fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excellent audio quality. And well acted out. I think you can genuinely feel the fear from the speaker. I do notice that there's a lot of silence between the start and end. We'll address it later in later parts. Lets play another random file, maybe a happy one this time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-23T10:07:19.315237Z",
     "iopub.status.busy": "2022-09-23T10:07:19.314812Z",
     "iopub.status.idle": "2022-09-23T10:07:19.785286Z",
     "shell.execute_reply": "2022-09-23T10:07:19.784457Z",
     "shell.execute_reply.started": "2022-09-23T10:07:19.315190Z"
    }
   },
   "outputs": [],
   "source": [
    "# Pick a happy track\n",
    "fname = RAV + 'Actor_14/03-01-03-02-02-02-14.wav'  \n",
    "data, sampling_rate = librosa.load(fname)\n",
    "plt.figure(figsize=(15, 5))\n",
    "librosa.display.waveplot(data, sr=sampling_rate)\n",
    "\n",
    "# Lets play the audio \n",
    "ipd.Audio(fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ravdess_con\"></a>\n",
    "### Conclusion\n",
    "...ok, to be fairly honest, I actual felt like that was a fearful tone at the start, up until the end. I had to play it 3 or 4 times to finally be convienced that it is indeed a happy sound. Looking at the wave plot between the 2 files, I notice the only real difference is the amplitute wherein this happy track has a higher amplituted at various points. But, could be coincidence, who knows at this stage. \n",
    "\n",
    "What I do know is that the audio quality is good and we need females in the dataset. If we don't include females, we'll end up with an AI that is bias towards / aagainst one gender, and its unethical. Unless there's a good reason, I'm not taking it out.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"tess\"></a>\n",
    "##  <center> 3. TESS dataset <center>\n",
    "Now on to the TESS dataset, its worth nothing that it's only based on 2 speakers, a young female and an older female. This should hopefully balance out the male dominant speakers that we have on SAVEE. \n",
    "\n",
    "Its got the same 7 key emotions we're interested in. But what is slightly different about this dataset compared to the previous two above, is that the addition of 'pleasant surprise' emotion. I haven't really checked to see for the RADVESS and SAVEE dataset, if the surpises are unpleasant. But I'm going to work with the assumption for now that its a pleasant surprise. If we find out from post modelling, surpise is highly inaccurate, we can come back and modify our assumption here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"tess_load\"></a>\n",
    "###  Load the dataset \n",
    "The speakers and the emotions are organised in seperate folders which is very convenient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-23T10:07:19.786770Z",
     "iopub.status.busy": "2022-09-23T10:07:19.786379Z",
     "iopub.status.idle": "2022-09-23T10:07:19.809293Z",
     "shell.execute_reply": "2022-09-23T10:07:19.808262Z",
     "shell.execute_reply.started": "2022-09-23T10:07:19.786723Z"
    }
   },
   "outputs": [],
   "source": [
    "dir_list = os.listdir(TESS)\n",
    "dir_list.sort()\n",
    "dir_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-23T10:07:19.810826Z",
     "iopub.status.busy": "2022-09-23T10:07:19.810430Z",
     "iopub.status.idle": "2022-09-23T10:07:20.898643Z",
     "shell.execute_reply": "2022-09-23T10:07:20.897929Z",
     "shell.execute_reply.started": "2022-09-23T10:07:19.810771Z"
    }
   },
   "outputs": [],
   "source": [
    "path = []\n",
    "emotion = []\n",
    "\n",
    "for i in dir_list:\n",
    "    fname = os.listdir(TESS + i)\n",
    "    for f in fname:\n",
    "        if i == 'OAF_angry' or i == 'YAF_angry':\n",
    "            emotion.append('female_angry')\n",
    "        elif i == 'OAF_disgust' or i == 'YAF_disgust':\n",
    "            emotion.append('female_disgust')\n",
    "        elif i == 'OAF_Fear' or i == 'YAF_fear':\n",
    "            emotion.append('female_fear')\n",
    "        elif i == 'OAF_happy' or i == 'YAF_happy':\n",
    "            emotion.append('female_happy')\n",
    "        elif i == 'OAF_neutral' or i == 'YAF_neutral':\n",
    "            emotion.append('female_neutral')                                \n",
    "        elif i == 'OAF_Pleasant_surprise' or i == 'YAF_pleasant_surprised':\n",
    "            emotion.append('female_surprise')               \n",
    "        elif i == 'OAF_Sad' or i == 'YAF_sad':\n",
    "            emotion.append('female_sad')\n",
    "        else:\n",
    "            emotion.append('Unknown')\n",
    "        path.append(TESS + i + \"/\" + f)\n",
    "\n",
    "TESS_df = pd.DataFrame(emotion, columns = ['labels'])\n",
    "TESS_df['source'] = 'TESS'\n",
    "TESS_df = pd.concat([TESS_df,pd.DataFrame(path, columns = ['path'])],axis=1)\n",
    "TESS_df.labels.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"tess_explore\"></a>\n",
    "### Explore the data\n",
    "400 files for each key emotion. Nice. So lets do the same thing again, take 2 audio files, play it and plot it to see what we're dealing with. Lets start with a fearful track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-23T10:07:20.900695Z",
     "iopub.status.busy": "2022-09-23T10:07:20.900116Z",
     "iopub.status.idle": "2022-09-23T10:07:21.632506Z",
     "shell.execute_reply": "2022-09-23T10:07:21.631514Z",
     "shell.execute_reply.started": "2022-09-23T10:07:20.900632Z"
    }
   },
   "outputs": [],
   "source": [
    "# lets play a fearful track \n",
    "fname = TESS + 'YAF_fear/YAF_dog_fear.wav' \n",
    "\n",
    "data, sampling_rate = librosa.load(fname)\n",
    "plt.figure(figsize=(15, 5))\n",
    "librosa.display.waveplot(data, sr=sampling_rate)\n",
    "\n",
    "# Lets play the audio \n",
    "ipd.Audio(fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "She sound almost the same as the female speaker from RAVDESS. Maybe when I have time I will go and investigate but for now,  the most important insight for me is that the expression of the emotions are very similar to RAVDESS and thus is a good indication it will serve as a good training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-23T10:07:21.634452Z",
     "iopub.status.busy": "2022-09-23T10:07:21.634067Z",
     "iopub.status.idle": "2022-09-23T10:07:22.398837Z",
     "shell.execute_reply": "2022-09-23T10:07:22.398009Z",
     "shell.execute_reply.started": "2022-09-23T10:07:21.634382Z"
    }
   },
   "outputs": [],
   "source": [
    "# lets play a happy track \n",
    "fname =  TESS + 'YAF_happy/YAF_dog_happy.wav' \n",
    "\n",
    "data, sampling_rate = librosa.load(fname)\n",
    "plt.figure(figsize=(15, 5))\n",
    "librosa.display.waveplot(data, sr=sampling_rate)\n",
    "\n",
    "# Lets play the audio \n",
    "ipd.Audio(fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"tess_con\"></a>\n",
    "### Conclusion\n",
    "Thats a beautiful happy voice, I played it a few times I must admit. Notice the amplitute is pretty high too on a few data points? We saw that on the RAVDESS dataset too. Perhaps that could be one of the few distinguishing factors? Because the speakers are the same, and the sentence uttered are the same, its an apples to apples comparison.  \n",
    "\n",
    "Again, who knows. For now what I do know is that the data quality is amazing and we're using it. The audio duration is about the same too hovering the 2 to 4 seconds mark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"crema\"></a>\n",
    "##  <center> 4. CREMA-D dataset <center>\n",
    "Last but not least, CREMA dataset. Not much is known about this dataset and I don't see much usage of this in general in the wild. But its a very large dataset which we need. And it has a good variety of different speakers, apparently taken from movies. And the speakers are of different ethnicities. This is good. Means better generalisation when we do transfer learning. Very important\n",
    "\n",
    "What we are missing from this dataset is the \"surprise\" emotion but no biggie, we can use the rest. But we have the rest. What's extra here is that it has different level of intensity of the emotion like RAVDESS. But we won't be using that for now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"crema_load\"></a>\n",
    "###  Load the dataset \n",
    "The speakers and the emotions like all previous datasets, are tagged in the audio filename itself. However, what we are missing is the Gender, which is kept as a seperate csv file that maps the actors. Instead of reading it and doing some matching, I'm just going to hardcode it here instead. Not the best practice but can do for now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-23T10:07:22.400372Z",
     "iopub.status.busy": "2022-09-23T10:07:22.399993Z",
     "iopub.status.idle": "2022-09-23T10:07:22.865056Z",
     "shell.execute_reply": "2022-09-23T10:07:22.863994Z",
     "shell.execute_reply.started": "2022-09-23T10:07:22.400329Z"
    }
   },
   "outputs": [],
   "source": [
    "dir_list = os.listdir(CREMA)\n",
    "dir_list.sort()\n",
    "print(dir_list[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-23T10:07:22.867318Z",
     "iopub.status.busy": "2022-09-23T10:07:22.866629Z",
     "iopub.status.idle": "2022-09-23T10:07:22.919097Z",
     "shell.execute_reply": "2022-09-23T10:07:22.917664Z",
     "shell.execute_reply.started": "2022-09-23T10:07:22.867257Z"
    }
   },
   "outputs": [],
   "source": [
    "gender = []\n",
    "emotion = []\n",
    "path = []\n",
    "female = [1002,1003,1004,1006,1007,1008,1009,1010,1012,1013,1018,1020,1021,1024,1025,1028,1029,1030,1037,1043,1046,1047,1049,\n",
    "          1052,1053,1054,1055,1056,1058,1060,1061,1063,1072,1073,1074,1075,1076,1078,1079,1082,1084,1089,1091]\n",
    "\n",
    "for i in dir_list: \n",
    "    part = i.split('_')\n",
    "    if int(part[0]) in female:\n",
    "        temp = 'female'\n",
    "    else:\n",
    "        temp = 'male'\n",
    "    gender.append(temp)\n",
    "    if part[2] == 'SAD' and temp == 'male':\n",
    "        emotion.append('male_sad')\n",
    "    elif part[2] == 'ANG' and temp == 'male':\n",
    "        emotion.append('male_angry')\n",
    "    elif part[2] == 'DIS' and temp == 'male':\n",
    "        emotion.append('male_disgust')\n",
    "    elif part[2] == 'FEA' and temp == 'male':\n",
    "        emotion.append('male_fear')\n",
    "    elif part[2] == 'HAP' and temp == 'male':\n",
    "        emotion.append('male_happy')\n",
    "    elif part[2] == 'NEU' and temp == 'male':\n",
    "        emotion.append('male_neutral')\n",
    "    elif part[2] == 'SAD' and temp == 'female':\n",
    "        emotion.append('female_sad')\n",
    "    elif part[2] == 'ANG' and temp == 'female':\n",
    "        emotion.append('female_angry')\n",
    "    elif part[2] == 'DIS' and temp == 'female':\n",
    "        emotion.append('female_disgust')\n",
    "    elif part[2] == 'FEA' and temp == 'female':\n",
    "        emotion.append('female_fear')\n",
    "    elif part[2] == 'HAP' and temp == 'female':\n",
    "        emotion.append('female_happy')\n",
    "    elif part[2] == 'NEU' and temp == 'female':\n",
    "        emotion.append('female_neutral')\n",
    "    else:\n",
    "        emotion.append('Unknown')\n",
    "    path.append(CREMA + i)\n",
    "    \n",
    "CREMA_df = pd.DataFrame(emotion, columns = ['labels'])\n",
    "CREMA_df['source'] = 'CREMA'\n",
    "CREMA_df = pd.concat([CREMA_df,pd.DataFrame(path, columns = ['path'])],axis=1)\n",
    "CREMA_df.labels.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"crema_explore\"></a>\n",
    "### Explore the data\n",
    "Ok so that's alot of data. Nice. Lets do the same thing again, take 2 audio files, play it and plot it to see what we're dealing with. Lets start with a happy track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-23T10:07:22.921905Z",
     "iopub.status.busy": "2022-09-23T10:07:22.921427Z",
     "iopub.status.idle": "2022-09-23T10:07:23.226994Z",
     "shell.execute_reply": "2022-09-23T10:07:23.226085Z",
     "shell.execute_reply.started": "2022-09-23T10:07:22.921815Z"
    }
   },
   "outputs": [],
   "source": [
    "# use the well known Librosa library for this task \n",
    "fname = CREMA + '1012_IEO_HAP_HI.wav'  \n",
    "data, sampling_rate = librosa.load(fname)\n",
    "plt.figure(figsize=(15, 5))\n",
    "librosa.display.waveplot(data, sr=sampling_rate)\n",
    "\n",
    "# Lets play the audio \n",
    "ipd.Audio(fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the audio itself is alittle echoey, for the lack of a better word. Its not as clear as what we've seen from the other data sets. And I'm not so sure if it'd consider the emotion happy. Sounds to me more like neutral. But then again, could be due to the audio quality. Lets listen to another one, a fearful one.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-23T10:07:23.229378Z",
     "iopub.status.busy": "2022-09-23T10:07:23.228960Z",
     "iopub.status.idle": "2022-09-23T10:07:23.542320Z",
     "shell.execute_reply": "2022-09-23T10:07:23.541595Z",
     "shell.execute_reply.started": "2022-09-23T10:07:23.229297Z"
    }
   },
   "outputs": [],
   "source": [
    "# A fearful track\n",
    "fname = CREMA + '1012_IEO_FEA_HI.wav'  \n",
    "data, sampling_rate = librosa.load(fname)\n",
    "plt.figure(figsize=(15, 5))\n",
    "librosa.display.waveplot(data, sr=sampling_rate)\n",
    "\n",
    "# Lets play the audio \n",
    "ipd.Audio(fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"crema_con\"></a>\n",
    "### Conclusion\n",
    "The same sentence uttered and quite convincing that its a fearful emotion there. Much clearer than the happy version. Now, I went back to listen a few more random tracks and what I noticed with this CREMA-D dataset is that its is highly varied in its quality. Some are crisp clear and some are really muffled or echoey. Also there's lots of silence as well. All in all, slightly 'dirtier' version of the data. But still good quality data none the less and we'll definitely use it. On the up side, a slightly noisy dataset would serve as a very good data augmentation by introducing noise to the pattern which we don't have so far. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"final\"></a>\n",
    "##  <center> 5. Final thoughts<center>\n",
    "All 4 dataset are good datasets. Having listen to them and doing some really rough inspections, I feel we can combine all of them. We need to anyway or else we will run into problems with overfitting. One of the issues that I see many other people before me who have made an attempt on an emotion classifier, they tend to stick to just one dataset. And whilst their hold-out set accuracy is high, they don't work well on new unseen dataset. \n",
    "    \n",
    "This is because, the classifier is trained on the same dataset and given the similar circumstances that the dataset was obtained or produced, (eg. audio quality, speaker repetition, duration and sentence uttered). To enable it to do well on new datasets, it needs to be given noise, make it work hard to find the real distinguishing characteristics of the emotion.\n",
    "\n",
    "Before we end it, final steps are to combine all the meta-data together as one. Remember we saved the paths for all the audio files. So this will be handy when we need to read all 4 data sources in different folder structures. \n",
    "\n",
    "Upvote this notebook if you like, and be sure to check out the other parts which are now available:\n",
    "* [Part 2 | Feature Extract](https://www.kaggle.com/ejlok1/audio-emotion-part-2-feature-extract)\n",
    "* [Part 3 | Baseline model](https://www.kaggle.com/ejlok1/audio-emotion-part-3-baseline-model)\n",
    "* [Part 4 | Apply to new audio data](https://www.kaggle.com/ejlok1/audio-emotion-part-4-apply-to-new-audio-data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-23T10:07:23.543790Z",
     "iopub.status.busy": "2022-09-23T10:07:23.543424Z",
     "iopub.status.idle": "2022-09-23T10:07:24.007656Z",
     "shell.execute_reply": "2022-09-23T10:07:24.006506Z",
     "shell.execute_reply.started": "2022-09-23T10:07:23.543748Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.concat([SAVEE_df, RAV_df, TESS_df, CREMA_df], axis = 0)\n",
    "print(df.labels.value_counts())\n",
    "df.head()\n",
    "df.to_csv(\"Data_path.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
