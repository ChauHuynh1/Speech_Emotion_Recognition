{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:#ffc0cb;font-size:70px;font-family:Georgia;text-align:center;\"><strong>Speech Emotion Recognition</strong></h1>\n",
    "\n",
    "### <b>Author Name: Nguyen Dang Huynh Chau</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:#ffc0cb;font-size:40px;font-family:Georgia;text-align:center;\"><strong> üìú Table of Content</strong></h1>\n",
    "\n",
    "### 1. [Data Preparation](#1)\n",
    "\n",
    "1.1 [Introduction](#1.1) \n",
    "\n",
    "1.2 [Importing Necessary Libraries and datasets](#1.2)\n",
    "\n",
    "1.3 [Data Retrieving](#1.3)\n",
    "\n",
    "<br>\n",
    "\n",
    "### 2. [Data Cleaning](#2)\n",
    "\n",
    "2.1 [About This Dataset](#2.1)\n",
    "\n",
    "2.2 [Data preprocessing](#2.2)\n",
    "\n",
    "> - 2.2.1 [Drop column `ID` and `Insurance`](#2.2.1)\n",
    "> - 2.2.2 [Rename column `Sepssis`](#2.2.2)  \n",
    "> - 2.2.3 [Convert `Sepsis` in to binary number](#2.2.3)\n",
    "> - 2.2.4 [Drop Duplicate](#2.2.4)  \n",
    "> - 2.2.5 [Convert Data Type](#2.2.5)  \n",
    "\n",
    "2.3 [Drop column](#2.3)\n",
    "\n",
    "> - 2.3.1 [Check correllation for dropping](#2.3.1)\n",
    "> - 2.3.2 [Check missing values for dropping](#2.3.2)  \n",
    "\n",
    "2.4 [Upper Case the content](#2.4)\n",
    "\n",
    "2.5 [Extra-whitespaces](#2.5)\n",
    "\n",
    "2.6 [Descriptive statistics for Central Tendency](#2.6)\n",
    "\n",
    "> - 2.6.1 [Overview statistics](#2.6.1)\n",
    "> - 2.6.2 [Domain Knowledge](#2.6.2)  \n",
    "> - 2.6.3 [Detect Outliers](#2.6.2)  \n",
    "\n",
    "2.7 [Save The Intermediate Data](#2.8)\n",
    "\n",
    "<br>\n",
    "\n",
    "### 3. [Data exploration (EDA)](#3)\n",
    "\n",
    "3.1 [Overall look on target variable](#3.1)\n",
    "\n",
    "> - 3.1.1 [Distribution of Sepsis](#3.1.1) \n",
    "> - 3.1.2 [Proportion of Sepsis](#3.1.1) \n",
    "\n",
    "3.2 [Frequency of each corresponiding Target variable type](#3.2)\n",
    "\n",
    "> - 3.2.1 [How old are they?](#3.2.1) \n",
    "> - 3.2.2 [How much they weight?](#3.2.2) \n",
    "> - 3.2.3 [How high PL (Blood Work Result-1 (mu U/ml)) that the Sepsis is likely to get?](#3.2.3) \n",
    "> - 3.2.4 [How high PR ((Blood Pressure (mm Hg)) that the Sepsis is likely to get?](#3.2.4) \n",
    "> - 3.2.5 [How high SK (Blood Work Result-2 (mm) that the Sepsis is likely to get?](#3.2.5) \n",
    "> - 3.2.6 [How high TS (Blood Work Result-3 (mu U/ml)) that the Sepsis is likely to get?](#3.2.6) \n",
    "> - 3.2.7 [How high BD2 (Blood Work Result-4 (mu U/ml)) that the Sepsis is likely to get?](#3.2.7) \n",
    "> - 3.2.8 [How high BD2 (Blood Work Result-4 (mu U/ml)) that the Sepsis is likely to get?](#3.2.8) \n",
    "> - 3.2.9 [Scatter matrix](#3.2.8) \n",
    "\n",
    "3.3 [Statistical Test for Correlation](#3.3)\n",
    "\n",
    "3.4 [Summary](#3.3)\n",
    "\n",
    "<br>\n",
    "\n",
    "### 4. [Feature Engineering](#4)\n",
    "\n",
    "4.1 [Class Imbalancing](#4.1)\n",
    "\n",
    "4.2 [Splitting the training data](#4.2)\n",
    "\n",
    "4.3 [Feature Scaling](#4.3)\n",
    "\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "### 5. [Model Building](#5) \n",
    "\n",
    "5.1 [Logistic Regression](#5.1)\n",
    "\n",
    "> - 5.1.1 [Train Model](#5.1.1)\n",
    "> - 5.1.2 [Model Evaluation](#5.1.2)\n",
    "> - 5.1.3 [Hypertuning parameter](#5.1.3)\n",
    "> - 5.1.4 [Retrain](#5.1.4)\n",
    "> - 5.1.5 [Conclusion](#5.1.5)\n",
    "\n",
    "5.2 [Decision Tree](#5.2)\n",
    "\n",
    "> - 5.2.1 [Train Model](#5.2.1)\n",
    "> - 5.2.2 [Hypertuning & Pruning](#5.2.2)\n",
    "\n",
    "> - 5.2.2.a [Post-Pruning](#5.2.2.a)\n",
    "> - 5.2.2.b [Pre-Pruning](#5.2.2.b)\n",
    "> - 5.2.2.c [Hypertuning parameter](#5.2.2.c)\n",
    "\n",
    "> - 5.2.3 [Hypertuning parameter](#5.2.3)\n",
    "> - 5.2.4 [Conclusion](#5.2.3)\n",
    "\n",
    "5.3 [Random Forest](#5.3)\n",
    "\n",
    "> - 5.3.1 [Train Model](#5.3.1)\n",
    "> - 5.3.2 [Model Evaluation](#5.3.2)\n",
    "> - 5.3.3 [Hypertuning parameter](#5.3.3)\n",
    "> - 5.3.4 [Retrain](#5.3.4)\n",
    "> - 5.3.5 [Conclusion](#5.3.5)\n",
    "\n",
    "<br>\n",
    "\n",
    "### 6. [Conculsions](#5)\n",
    "\n",
    "<br>\n",
    "\n",
    "### 7. [References](#7)\n",
    "\n",
    "<br>\n",
    "\n",
    "### 8. [Appendix](#8)\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a>\n",
    "<h1 style=\"color:#ffc0cb;font-size:40px;font-family:Georgia;text-align:center;\"><strong> ‚úçÔ∏è 1. Data Preparation</strong></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1.1\"></a>\n",
    "# Ô∏èüéØ 1.1 Introduction\n",
    "****\n",
    "\n",
    "<p style=\"list-style-type:circle;font-family:Yu Mincho Demibold;font-size:20px;color:black\"> Speech Emotion Recognition, or SER for short, is the process of trying to identify affective and emotional states in speech. This makes use of the fact that tone and pitch in the voice frequently convey underlying emotion. In order to comprehend human emotion, animals like dogs and horses also use this phenomenon. The component of speech recognition that is growing in popularity and demand is emotion recognition. Although there are ways to identify emotions from data using machine learning methods, this research tries to use deep learning to do so. SER (Speech Emotion Recognition) is used in contact centers to categorize conversations based on emotions and can be used as a performance metric for conversational analysis to identify the dissatisfied client, customer satisfaction, and other factors that can help businesses improve their services. It can also be utilized as an in-car board system based on information about the driver's mental state to ensure the driver's safety and prevent accidents.</p>\n",
    "\n",
    "## üì£ What datasets are used in this project?\n",
    "\n",
    "<ul style=\"list-style-type:circle;font-family:Yu Mincho Demibold;font-size:20px;color:black\">\n",
    "    <li> Crowd-sourced Emotional Mutimodal Actors Dataset (Crema-D) </li>\n",
    "    <li> Ryerson Audio-Visual Database of Emotional Speech and Song (Ravdess) </li>\n",
    "    <li> Surrey Audio-Visual Expressed Emotion (Savee) </li>\n",
    "    <li> Toronto emotional speech set (Tess) </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1.2\"></a>\n",
    "# ‚ú¥Ô∏è 1.2 Importing Necessary Libraries and datasets\n",
    "****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "execution": {
     "iopub.execute_input": "2022-08-04T06:29:12.031189Z",
     "iopub.status.busy": "2022-08-04T06:29:12.030803Z",
     "iopub.status.idle": "2022-08-04T06:29:12.126776Z",
     "shell.execute_reply": "2022-08-04T06:29:12.124685Z",
     "shell.execute_reply.started": "2022-08-04T06:29:12.031155Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# librosa is a Python library for analyzing audio and music. It can be used to extract the data from the audio files we will see it later.\n",
    "import librosa\n",
    "import librosa.display\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# to play the audio files\n",
    "from IPython.display import Audio\n",
    "\n",
    "import keras\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv1D, MaxPooling1D, Flatten, Dropout, BatchNormalization\n",
    "from keras.utils import np_utils, to_categorical\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "import warnings\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check numpy and pandas version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numpy version:  1.22.3\n",
      "Pandas version:  1.5.0\n",
      "Python 3.10.4\r\n"
     ]
    }
   ],
   "source": [
    "# check the version of the packages\n",
    "print(\"Numpy version: \", np.__version__)\n",
    "print(\"Pandas version: \",pd.__version__)\n",
    "! python --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ------------> OBSERVATION\n",
    "***\n",
    "I want to check the numpy and pandas version since I want to make sure the verson is appropriate for my work load. Currently, it is still appropriate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1.3\"></a>\n",
    "# üì≤ 1.3 Data Retrieving\n",
    "***\n",
    "<ul style=\"list-style-type:circle;font-family:Yu Mincho Demibold;font-size:20px;color:black\">\n",
    "    <li> I'll be developing a dataframe that stores all of the data's emotions along with their pathways because we're working with four different datasets. </li>\n",
    "    <li> From this dataframe, we will extract features to use in the training of our model.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-08-04T06:27:37.581848Z",
     "iopub.status.idle": "2022-08-04T06:27:37.582335Z"
    }
   },
   "outputs": [],
   "source": [
    "# Paths for data.\n",
    "Ravdess = \"ravdess/\"\n",
    "Crema = \"cremad\"\n",
    "Tess = \"tess\"\n",
    "Savee = \"savee\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Actor_16', 'Actor_11', 'Actor_18', 'Actor_20', 'Actor_21']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "#    for filename in filenames:\n",
    "#        print(os.path.join(dirname, filename))\n",
    "\n",
    "TESS = \"/kaggle/input/toronto-emotional-speech-set-tess/tess toronto emotional speech set data/TESS Toronto emotional speech set data/\"\n",
    "RAV = \"ravdess/\"\n",
    "SAVEE = \"/kaggle/input/surrey-audiovisual-expressed-emotion-savee/ALL/\"\n",
    "CREMA = \"/kaggle/input/cremad/AudioWAV/\"\n",
    "\n",
    "# Run one example \n",
    "dir_list = os.listdir(RAV)\n",
    "dir_list[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  <center> 1. Ravdess Dataframe <center>\n",
    "Here is the filename identifiers as per the official RAVDESS website:\n",
    "\n",
    "* Modality (01 = full-AV, 02 = video-only, 03 = audio-only).\n",
    "* Vocal channel (01 = speech, 02 = song).\n",
    "* Emotion (01 = neutral, 02 = calm, 03 = happy, 04 = sad, 05 = angry, 06 = fearful, 07 = disgust, 08 = surprised).\n",
    "* Emotional intensity (01 = normal, 02 = strong). NOTE: There is no strong intensity for the 'neutral' emotion.\n",
    "* Statement (01 = \"Kids are talking by the door\", 02 = \"Dogs are sitting by the door\").\n",
    "* Repetition (01 = 1st repetition, 02 = 2nd repetition).\n",
    "* Actor (01 to 24. Odd numbered actors are male, even numbered actors are female).\n",
    "\n",
    "So, here's an example of an audio filename. 02-01-06-01-02-01-12.mp4\n",
    "This means the meta data for the audio file is:\n",
    "\n",
    "* Video-only (02)\n",
    "* Speech (01)\n",
    "* Fearful (06)\n",
    "* Normal intensity (01)\n",
    "* Statement \"dogs\" (02)\n",
    "* 1st Repetition (01)\n",
    "* 12th Actor (12) - Female (as the actor ID number is even)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Actor_16', 'Actor_11', 'Actor_18', 'Actor_20', 'Actor_21', 'Actor_19', 'Actor_10', 'Actor_17', '.DS_Store', 'Actor_04', 'Actor_03', 'Actor_02', 'Actor_05', 'audio_speech_actors_01-24', 'Actor_12', 'Actor_15', 'Actor_23', 'Actor_24', 'Actor_22', 'Actor_14', 'Actor_13', 'Actor_09', 'Actor_07', 'Actor_06', 'Actor_01', 'Actor_08']\n",
      "03\n"
     ]
    }
   ],
   "source": [
    "ravdess_directory_list = os.listdir(Ravdess)\n",
    "\n",
    "file_emotion = []\n",
    "file_path = []\n",
    "\n",
    "# as their are 20 different actors in our previous directory we need to extract files for each actor.\n",
    "actor = os.listdir(Ravdess)\n",
    "print(actor)\n",
    "\n",
    "part = actor[10].split('.')[0]\n",
    "part = actor[10].split('_')\n",
    "\n",
    "# print(part[1])\n",
    "\n",
    "if(actor[8] == \".DS_Store\"):\n",
    "    print(part[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [62]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m fname:\n\u001b[1;32m     10\u001b[0m     part \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 11\u001b[0m     emotion\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mint\u001b[39m(\u001b[43mpart\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m))\n\u001b[1;32m     12\u001b[0m     temp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(part[\u001b[38;5;241m6\u001b[39m])\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m temp\u001b[38;5;241m%\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "dir_list = os.listdir(RAV)\n",
    "dir_list.sort()\n",
    "\n",
    "emotion = []\n",
    "gender = []\n",
    "path = []\n",
    "for i in dir_list:\n",
    "    fname = os.listdir(RAV + i)\n",
    "    for f in fname:\n",
    "        part = f.split('.')[0].split('-')\n",
    "        emotion.append(int(part[2]))\n",
    "        temp = int(part[6])\n",
    "        if temp%2 == 0:\n",
    "            temp = \"female\"\n",
    "        else:\n",
    "            temp = \"male\"\n",
    "        gender.append(temp)\n",
    "        path.append(RAV + i + '/' + f)\n",
    "\n",
    "        \n",
    "RAV_df = pd.DataFrame(emotion)\n",
    "RAV_df = RAV_df.replace({1:'neutral', 2:'neutral', 3:'happy', 4:'sad', 5:'angry', 6:'fear', 7:'disgust', 8:'surprise'})\n",
    "RAV_df = pd.concat([pd.DataFrame(gender),RAV_df],axis=1)\n",
    "RAV_df.columns = ['gender','emotion']\n",
    "RAV_df['labels'] =RAV_df.gender + '_' + RAV_df.emotion\n",
    "RAV_df['source'] = 'RAVDESS'  \n",
    "RAV_df = pd.concat([RAV_df,pd.DataFrame(path, columns = ['path'])],axis=1)\n",
    "RAV_df = RAV_df.drop(['gender', 'emotion'], axis=1)\n",
    "RAV_df.labels.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-08-04T06:27:37.584286Z",
     "iopub.status.idle": "2022-08-04T06:27:37.585268Z"
    }
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [51]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m actor \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mlistdir(Ravdess)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m actor:\n\u001b[0;32m----> 9\u001b[0m     part \u001b[38;5;241m=\u001b[39m \u001b[43mfile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     10\u001b[0m     part \u001b[38;5;241m=\u001b[39m part\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     11\u001b[0m         \u001b[38;5;66;03m# third part in each file represents the emotion associated to that file.\u001b[39;00m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "ravdess_directory_list = os.listdir(Ravdess)\n",
    "\n",
    "file_emotion = []\n",
    "file_path = []\n",
    "for dir in ravdess_directory_list:\n",
    "    # as their are 20 different actors in our previous directory we need to extract files for each actor.\n",
    "    actor = os.listdir(Ravdess)\n",
    "    for file in actor:\n",
    "        part = file.split('.')[1]\n",
    "        part = part.split('_')\n",
    "            # third part in each file represents the emotion associated to that file.\n",
    "        file_emotion.append(int(part[0]))\n",
    "        file_path.append(Ravdess + dir + '/' + file)\n",
    "        if(actor == \".DS_Store\"):\n",
    "            continue\n",
    "        \n",
    "        \n",
    "# dataframe for emotion of files\n",
    "emotion_df = pd.DataFrame(file_emotion, columns=['Emotions'])\n",
    "\n",
    "# dataframe for path of files.\n",
    "path_df = pd.DataFrame(file_path, columns=['Path'])\n",
    "Ravdess_df = pd.concat([emotion_df, path_df], axis=1)\n",
    "\n",
    "# changing integers to actual emotions.\n",
    "Ravdess_df.Emotions.replace({1:'neutral', 2:'calm', 3:'happy', 4:'sad', 5:'angry', 6:'fear', 7:'disgust', 8:'surprise'}, inplace=True)\n",
    "Ravdess_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Ravdess_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [25]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mRavdess_df\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEmotions\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalue_counts()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Ravdess_df' is not defined"
     ]
    }
   ],
   "source": [
    "Ravdess_df['Emotions'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>2. Crema DataFrame</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-08-04T06:27:37.587357Z",
     "iopub.status.idle": "2022-08-04T06:27:37.588277Z"
    }
   },
   "outputs": [],
   "source": [
    "crema_directory_list = os.listdir(Crema)\n",
    "\n",
    "file_emotion = []\n",
    "file_path = []\n",
    "\n",
    "for file in crema_directory_list:\n",
    "    # storing file paths\n",
    "    file_path.append(Crema + file)\n",
    "    # storing file emotions\n",
    "    part=file.split('_')\n",
    "    if part[2] == 'SAD':\n",
    "        file_emotion.append('sad')\n",
    "    elif part[2] == 'ANG':\n",
    "        file_emotion.append('angry')\n",
    "    elif part[2] == 'DIS':\n",
    "        file_emotion.append('disgust')\n",
    "    elif part[2] == 'FEA':\n",
    "        file_emotion.append('fear')\n",
    "    elif part[2] == 'HAP':\n",
    "        file_emotion.append('happy')\n",
    "    elif part[2] == 'NEU':\n",
    "        file_emotion.append('neutral')\n",
    "    else:\n",
    "        file_emotion.append('Unknown')\n",
    "        \n",
    "# dataframe for emotion of files\n",
    "emotion_df = pd.DataFrame(file_emotion, columns=['Emotions'])\n",
    "\n",
    "# dataframe for path of files.\n",
    "path_df = pd.DataFrame(file_path, columns=['Path'])\n",
    "Crema_df = pd.concat([emotion_df, path_df], axis=1)\n",
    "Crema_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Crema_df['Emotions'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  <center> 3. TESS dataset <center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-08-04T06:27:37.590776Z",
     "iopub.status.idle": "2022-08-04T06:27:37.591827Z"
    }
   },
   "outputs": [],
   "source": [
    "tess_directory_list = os.listdir(Tess)\n",
    "\n",
    "file_emotion = []\n",
    "file_path = []\n",
    "\n",
    "for dir in tess_directory_list:\n",
    "    directories = os.listdir(Tess + dir)\n",
    "    for file in directories:\n",
    "        part = file.split('.')[0]\n",
    "#         print(part)\n",
    "        part = part.split('_')\n",
    "        if(len(part) >2):\n",
    "            part = part[2]\n",
    "        else:\n",
    "            part = part[1]\n",
    "        if part=='ps':\n",
    "            file_emotion.append('surprise')\n",
    "        else:\n",
    "            file_emotion.append(part)\n",
    "        file_path.append(Tess + dir + '/' + file)\n",
    "        \n",
    "# dataframe for emotion of files\n",
    "emotion_df = pd.DataFrame(file_emotion, columns=['Emotions'])\n",
    "\n",
    "# dataframe for path of files.\n",
    "path_df = pd.DataFrame(file_path, columns=['Path'])\n",
    "Tess_df = pd.concat([emotion_df, path_df], axis=1)\n",
    "Tess_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tess_df['Emotions'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  <center> 4. CREMA-D dataset <center>\n",
    "The audio files in this dataset are named in such a way that the prefix letters describes the emotion classes as follows:\n",
    "\n",
    "* 'a' = 'anger'\n",
    "* 'd' = 'disgust'\n",
    "* 'f' = 'fear'\n",
    "* 'h' = 'happiness'\n",
    "* 'n' = 'neutral'\n",
    "* 'sa' = 'sadness'\n",
    "* 'su' = 'surprise'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-08-04T06:27:37.593669Z",
     "iopub.status.idle": "2022-08-04T06:27:37.594744Z"
    }
   },
   "outputs": [],
   "source": [
    "savee_directory_list = os.listdir(Savee)\n",
    "\n",
    "file_emotion = []\n",
    "file_path = []\n",
    "\n",
    "for file in savee_directory_list:\n",
    "    file_path.append(Savee + file)\n",
    "    part = file.split('_')[1]\n",
    "    ele = part[:-6]\n",
    "    if ele=='a':\n",
    "        file_emotion.append('angry')\n",
    "    elif ele=='d':\n",
    "        file_emotion.append('disgust')\n",
    "    elif ele=='f':\n",
    "        file_emotion.append('fear')\n",
    "    elif ele=='h':\n",
    "        file_emotion.append('happy')\n",
    "    elif ele=='n':\n",
    "        file_emotion.append('neutral')\n",
    "    elif ele=='sa':\n",
    "        file_emotion.append('sad')\n",
    "    else:\n",
    "        file_emotion.append('surprise')\n",
    "        \n",
    "# dataframe for emotion of files\n",
    "emotion_df = pd.DataFrame(file_emotion, columns=['Emotions'])\n",
    "\n",
    "# dataframe for path of files.\n",
    "path_df = pd.DataFrame(file_path, columns=['Path'])\n",
    "Savee_df = pd.concat([emotion_df, path_df], axis=1)\n",
    "Savee_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Savee_df['Emotions'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-08-04T06:27:37.597250Z",
     "iopub.status.idle": "2022-08-04T06:27:37.598264Z"
    }
   },
   "outputs": [],
   "source": [
    "# creating Dataframe using all the 4 dataframes we created so far.\n",
    "data_path = pd.concat([Ravdess_df, Crema_df, Tess_df, Savee_df], axis = 0)\n",
    "data_path.to_csv(\"data_path.csv\",index=False)\n",
    "data_path.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2\"></a>\n",
    "<h1 style=\"color:#ffc0cb;font-size:40px;font-family:Georgia;text-align:center;\"><strong> üßπ 2. Data Cleaning</strong></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2.1\"></a>\n",
    "# ‚ùå 2.1 Check Typo\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path['Emotions'] = data_path['Emotions'].str.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path['Emotions'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path.loc[data_path['Emotions'].isin(['SURPRISED']), 'Emotions'] = 'SURPRISE'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualisation and Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's plot the count of each emotions in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-08-04T06:27:37.600278Z",
     "iopub.status.idle": "2022-08-04T06:27:37.601137Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.title('Count of Emotions', size=16)\n",
    "sns.countplot(data_path.Emotions)\n",
    "plt.ylabel('Count', size=12)\n",
    "plt.xlabel('Emotions', size=12)\n",
    "sns.despine(top=True, right=True, left=False, bottom=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also plot waveplots and spectograms for audio signals\n",
    "\n",
    "* Waveplots - Waveplots let us know the loudness of the audio at a given time.\n",
    "* Spectograms - A spectrogram is a visual representation of the spectrum of frequencies of sound or other signals as they vary with time. It‚Äôs a representation of frequencies changing with respect to time for given audio/music signals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-08-04T06:27:37.602335Z",
     "iopub.status.idle": "2022-08-04T06:27:37.604012Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_waveplot(data, sr, e):\n",
    "    plt.figure(figsize=(10, 3))\n",
    "    plt.title('Waveplot for audio with {} emotion'.format(e), size=15)\n",
    "    librosa.display.waveplot(data, sr=sr)\n",
    "    plt.show()\n",
    "\n",
    "def create_spectrogram(data, sr, e):\n",
    "    # stft function converts the data into short term fourier transform\n",
    "    X = librosa.stft(data)\n",
    "    Xdb = librosa.amplitude_to_db(abs(X))\n",
    "    plt.figure(figsize=(12, 3))\n",
    "    plt.title('Spectrogram for audio with {} emotion'.format(e), size=15)\n",
    "    librosa.display.specshow(Xdb, sr=sr, x_axis='time', y_axis='hz')   \n",
    "    #librosa.display.specshow(Xdb, sr=sr, x_axis='time', y_axis='log')\n",
    "    plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-08-04T06:27:37.605680Z",
     "iopub.status.idle": "2022-08-04T06:27:37.606759Z"
    }
   },
   "outputs": [],
   "source": [
    "emotion='fear'\n",
    "path = np.array(data_path.Path[data_path.Emotions==emotion])[1]\n",
    "data, sampling_rate = librosa.load(path)\n",
    "create_waveplot(data, sampling_rate, emotion)\n",
    "create_spectrogram(data, sampling_rate, emotion)\n",
    "Audio(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-08-04T06:27:37.608404Z",
     "iopub.status.idle": "2022-08-04T06:27:37.609497Z"
    }
   },
   "outputs": [],
   "source": [
    "emotion='angry'\n",
    "path = np.array(data_path.Path[data_path.Emotions==emotion])[1]\n",
    "data, sampling_rate = librosa.load(path)\n",
    "create_waveplot(data, sampling_rate, emotion)\n",
    "create_spectrogram(data, sampling_rate, emotion)\n",
    "Audio(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-08-04T06:27:37.611189Z",
     "iopub.status.idle": "2022-08-04T06:27:37.612275Z"
    }
   },
   "outputs": [],
   "source": [
    "emotion='sad'\n",
    "path = np.array(data_path.Path[data_path.Emotions==emotion])[1]\n",
    "data, sampling_rate = librosa.load(path)\n",
    "create_waveplot(data, sampling_rate, emotion)\n",
    "create_spectrogram(data, sampling_rate, emotion)\n",
    "Audio(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-08-04T06:27:37.614116Z",
     "iopub.status.idle": "2022-08-04T06:27:37.615221Z"
    }
   },
   "outputs": [],
   "source": [
    "emotion='happy'\n",
    "path = np.array(data_path.Path[data_path.Emotions==emotion])[1]\n",
    "data, sampling_rate = librosa.load(path)\n",
    "create_waveplot(data, sampling_rate, emotion)\n",
    "create_spectrogram(data, sampling_rate, emotion)\n",
    "Audio(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation\n",
    "\n",
    "- Data augmentation is the process by which we create new synthetic data samples by adding small perturbations on our initial training set.\n",
    "- To generate syntactic data for audio, we can apply noise injection, shifting time, changing pitch and speed.\n",
    "- The objective is to make our model invariant to those perturbations and enhace its ability to generalize.\n",
    "- In order to this to work adding the perturbations must conserve the same label as the original training sample.\n",
    "- In images data augmention can be performed by shifting the image, zooming, rotating ...\n",
    "\n",
    "First, let's check which augmentation techniques works better for our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-08-04T06:27:37.616942Z",
     "iopub.status.idle": "2022-08-04T06:27:37.618074Z"
    }
   },
   "outputs": [],
   "source": [
    "def noise(data):\n",
    "    noise_amp = 0.035*np.random.uniform()*np.amax(data)\n",
    "    data = data + noise_amp*np.random.normal(size=data.shape[0])\n",
    "    return data\n",
    "\n",
    "def stretch(data, rate=0.8):\n",
    "    return librosa.effects.time_stretch(data, rate)\n",
    "\n",
    "def shift(data):\n",
    "    shift_range = int(np.random.uniform(low=-5, high = 5)*1000)\n",
    "    return np.roll(data, shift_range)\n",
    "\n",
    "def pitch(data, sampling_rate, pitch_factor=0.7):\n",
    "    return librosa.effects.pitch_shift(data, sampling_rate, pitch_factor)\n",
    "\n",
    "# taking any example and checking for techniques.\n",
    "path = np.array(data_path.Path)[1]\n",
    "data, sample_rate = librosa.load(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Simple Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-08-04T06:27:37.619782Z",
     "iopub.status.idle": "2022-08-04T06:27:37.620889Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,4))\n",
    "librosa.display.waveplot(y=data, sr=sample_rate)\n",
    "Audio(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Noise Injection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-08-04T06:27:37.622814Z",
     "iopub.status.idle": "2022-08-04T06:27:37.624017Z"
    }
   },
   "outputs": [],
   "source": [
    "x = noise(data)\n",
    "plt.figure(figsize=(14,4))\n",
    "librosa.display.waveplot(y=x, sr=sample_rate)\n",
    "Audio(x, rate=sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see noise injection is a very good augmentation technique because of which we can assure our training model is not overfitted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Stretching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-08-04T06:27:37.625863Z",
     "iopub.status.idle": "2022-08-04T06:27:37.627031Z"
    }
   },
   "outputs": [],
   "source": [
    "x = stretch(data)\n",
    "plt.figure(figsize=(14,4))\n",
    "librosa.display.waveplot(y=x, sr=sample_rate)\n",
    "Audio(x, rate=sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Shifting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-08-04T06:27:37.628979Z",
     "iopub.status.idle": "2022-08-04T06:27:37.630166Z"
    }
   },
   "outputs": [],
   "source": [
    "x = shift(data)\n",
    "plt.figure(figsize=(14,4))\n",
    "librosa.display.waveplot(y=x, sr=sample_rate)\n",
    "Audio(x, rate=sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Pitch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-08-04T06:27:37.632031Z",
     "iopub.status.idle": "2022-08-04T06:27:37.633181Z"
    }
   },
   "outputs": [],
   "source": [
    "x = pitch(data, sample_rate)\n",
    "plt.figure(figsize=(14,4))\n",
    "librosa.display.waveplot(y=x, sr=sample_rate)\n",
    "Audio(x, rate=sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- From the above types of augmentation techniques i am using noise, stretching(ie. changing speed) and some pitching."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction\n",
    "- Extraction of features is a very important part in analyzing and finding relations between different things. As we already know that the data provided of audio cannot be understood by the models directly so we need to convert them into an understandable format for which feature extraction is used.\n",
    "\n",
    "\n",
    "The audio signal is a three-dimensional signal in which three axes represent time, amplitude and frequency.\n",
    "\n",
    "![image.png](https://miro.medium.com/max/633/1*7sKM9aECRmuoqTadCYVw9A.jpeg)\n",
    "\n",
    "I am no expert on audio signals and feature extraction on audio files so i need to search and found a very good blog written by [Askash Mallik](https://medium.com/heuristics/audio-signal-feature-extraction-and-clustering-935319d2225) on feature extraction.\n",
    "\n",
    "As stated there with the help of the sample rate and the sample data, one can perform several transformations on it to extract valuable features out of it.\n",
    "1. Zero Crossing Rate : The rate of sign-changes of the signal during the duration of a particular frame.\n",
    "2. Energy : The sum of squares of the signal values, normalized by the respective frame length.\n",
    "3. Entropy of Energy : The entropy of sub-frames‚Äô normalized energies. It can be interpreted as a measure of abrupt changes.\n",
    "4. Spectral Centroid : The center of gravity of the spectrum.\n",
    "5. Spectral Spread : The second central moment of the spectrum.\n",
    "6. Spectral Entropy :  Entropy of the normalized spectral energies for a set of sub-frames.\n",
    "7. Spectral Flux : The squared difference between the normalized magnitudes of the spectra of the two successive frames.\n",
    "8. Spectral Rolloff : The frequency below which 90% of the magnitude distribution of the spectrum is concentrated.\n",
    "9.  MFCCs Mel Frequency Cepstral Coefficients form a cepstral representation where the frequency bands are not linear but distributed according to the mel-scale.\n",
    "10. Chroma Vector : A 12-element representation of the spectral energy where the bins represent the 12 equal-tempered pitch classes of western-type music (semitone spacing).\n",
    "11. Chroma Deviation : The standard deviation of the 12 chroma coefficients.\n",
    "\n",
    "\n",
    "In this project i am not going deep in feature selection process to check which features are good for our dataset rather i am only extracting 5 features:\n",
    "- Zero Crossing Rate\n",
    "- Chroma_stft\n",
    "- MFCC\n",
    "- RMS(root mean square) value\n",
    "- MelSpectogram to train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-08-04T06:27:37.634930Z",
     "iopub.status.idle": "2022-08-04T06:27:37.636152Z"
    }
   },
   "outputs": [],
   "source": [
    "def extract_features(data):\n",
    "    # ZCR\n",
    "    result = np.array([])\n",
    "    zcr = np.mean(librosa.feature.zero_crossing_rate(y=data).T, axis=0)\n",
    "    result=np.hstack((result, zcr)) # stacking horizontally\n",
    "\n",
    "    # Chroma_stft\n",
    "    stft = np.abs(librosa.stft(data))\n",
    "    chroma_stft = np.mean(librosa.feature.chroma_stft(S=stft, sr=sample_rate).T, axis=0)\n",
    "    result = np.hstack((result, chroma_stft)) # stacking horizontally\n",
    "\n",
    "    # MFCC\n",
    "    mfcc = np.mean(librosa.feature.mfcc(y=data, sr=sample_rate).T, axis=0)\n",
    "    result = np.hstack((result, mfcc)) # stacking horizontally\n",
    "\n",
    "    # Root Mean Square Value\n",
    "    rms = np.mean(librosa.feature.rms(y=data).T, axis=0)\n",
    "    result = np.hstack((result, rms)) # stacking horizontally\n",
    "\n",
    "    # MelSpectogram\n",
    "    mel = np.mean(librosa.feature.melspectrogram(y=data, sr=sample_rate).T, axis=0)\n",
    "    result = np.hstack((result, mel)) # stacking horizontally\n",
    "    \n",
    "    return result\n",
    "\n",
    "def get_features(path):\n",
    "    # duration and offset are used to take care of the no audio in start and the ending of each audio files as seen above.\n",
    "    data, sample_rate = librosa.load(path, duration=2.5, offset=0.6)\n",
    "    \n",
    "    # without augmentation\n",
    "    res1 = extract_features(data)\n",
    "    result = np.array(res1)\n",
    "    \n",
    "    # data with noise\n",
    "    noise_data = noise(data)\n",
    "    res2 = extract_features(noise_data)\n",
    "    result = np.vstack((result, res2)) # stacking vertically\n",
    "    \n",
    "    # data with stretching and pitching\n",
    "    new_data = stretch(data)\n",
    "    data_stretch_pitch = pitch(new_data, sample_rate)\n",
    "    res3 = extract_features(data_stretch_pitch)\n",
    "    result = np.vstack((result, res3)) # stacking vertically\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-08-04T06:27:37.637944Z",
     "iopub.status.idle": "2022-08-04T06:27:37.639019Z"
    }
   },
   "outputs": [],
   "source": [
    "X, Y = [], []\n",
    "for path, emotion in zip(data_path.Path, data_path.Emotions):\n",
    "    feature = get_features(path)\n",
    "    for ele in feature:\n",
    "        X.append(ele)\n",
    "        # appending emotion 3 times as we have made 3 augmentation techniques on each audio file.\n",
    "        Y.append(emotion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-08-04T06:27:37.640679Z",
     "iopub.status.idle": "2022-08-04T06:27:37.641794Z"
    }
   },
   "outputs": [],
   "source": [
    "len(X), len(Y), data_path.Path.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-08-04T06:27:37.643612Z",
     "iopub.status.idle": "2022-08-04T06:27:37.644767Z"
    }
   },
   "outputs": [],
   "source": [
    "Features = pd.DataFrame(X)\n",
    "Features['labels'] = Y\n",
    "Features.to_csv('features.csv', index=False)\n",
    "Features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We have applied data augmentation and extracted the features for each audio files and saved them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "- As of now we have extracted the data, now we need to normalize and split our data for training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-08-04T06:27:37.646411Z",
     "iopub.status.idle": "2022-08-04T06:27:37.647571Z"
    }
   },
   "outputs": [],
   "source": [
    "X = Features.iloc[: ,:-1].values\n",
    "Y = Features['labels'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-08-04T06:27:37.649320Z",
     "iopub.status.idle": "2022-08-04T06:27:37.650393Z"
    }
   },
   "outputs": [],
   "source": [
    "# As this is a multiclass classification problem onehotencoding our Y.\n",
    "encoder = OneHotEncoder()\n",
    "Y = encoder.fit_transform(np.array(Y).reshape(-1,1)).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-08-04T06:27:37.652056Z",
     "iopub.status.idle": "2022-08-04T06:27:37.653138Z"
    }
   },
   "outputs": [],
   "source": [
    "# splitting data\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, random_state=0, shuffle=True)\n",
    "x_train.shape, y_train.shape, x_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-08-04T06:27:37.654823Z",
     "iopub.status.idle": "2022-08-04T06:27:37.655892Z"
    }
   },
   "outputs": [],
   "source": [
    "# scaling our data with sklearn's Standard scaler\n",
    "scaler = StandardScaler()\n",
    "x_train = scaler.fit_transform(x_train)\n",
    "x_test = scaler.transform(x_test)\n",
    "x_train.shape, y_train.shape, x_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-08-04T06:27:37.657531Z",
     "iopub.status.idle": "2022-08-04T06:27:37.658601Z"
    }
   },
   "outputs": [],
   "source": [
    "# making our data compatible to model.\n",
    "x_train = np.expand_dims(x_train, axis=2)\n",
    "x_test = np.expand_dims(x_test, axis=2)\n",
    "x_train.shape, y_train.shape, x_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-08-04T06:27:37.660293Z",
     "iopub.status.idle": "2022-08-04T06:27:37.661440Z"
    }
   },
   "outputs": [],
   "source": [
    "model=Sequential()\n",
    "model.add(Conv1D(256, kernel_size=5, strides=1, padding='same', activation='relu', input_shape=(x_train.shape[1], 1)))\n",
    "model.add(MaxPooling1D(pool_size=5, strides = 2, padding = 'same'))\n",
    "\n",
    "model.add(Conv1D(256, kernel_size=5, strides=1, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=5, strides = 2, padding = 'same'))\n",
    "\n",
    "model.add(Conv1D(128, kernel_size=5, strides=1, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=5, strides = 2, padding = 'same'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv1D(64, kernel_size=5, strides=1, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=5, strides = 2, padding = 'same'))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=32, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Dense(units=8, activation='softmax'))\n",
    "model.compile(optimizer = 'adam' , loss = 'categorical_crossentropy' , metrics = ['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-08-04T06:27:37.663126Z",
     "iopub.status.idle": "2022-08-04T06:27:37.664181Z"
    }
   },
   "outputs": [],
   "source": [
    "rlrp = ReduceLROnPlateau(monitor='loss', factor=0.4, verbose=0, patience=2, min_lr=0.0000001)\n",
    "history=model.fit(x_train, y_train, batch_size=64, epochs=50, validation_data=(x_test, y_test), callbacks=[rlrp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-08-04T06:27:37.666005Z",
     "iopub.status.idle": "2022-08-04T06:27:37.667119Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Accuracy of our model on test data : \" , model.evaluate(x_test,y_test)[1]*100 , \"%\")\n",
    "\n",
    "epochs = [i for i in range(50)]\n",
    "fig , ax = plt.subplots(1,2)\n",
    "train_acc = history.history['accuracy']\n",
    "train_loss = history.history['loss']\n",
    "test_acc = history.history['val_accuracy']\n",
    "test_loss = history.history['val_loss']\n",
    "\n",
    "fig.set_size_inches(20,6)\n",
    "ax[0].plot(epochs , train_loss , label = 'Training Loss')\n",
    "ax[0].plot(epochs , test_loss , label = 'Testing Loss')\n",
    "ax[0].set_title('Training & Testing Loss')\n",
    "ax[0].legend()\n",
    "ax[0].set_xlabel(\"Epochs\")\n",
    "\n",
    "ax[1].plot(epochs , train_acc , label = 'Training Accuracy')\n",
    "ax[1].plot(epochs , test_acc , label = 'Testing Accuracy')\n",
    "ax[1].set_title('Training & Testing Accuracy')\n",
    "ax[1].legend()\n",
    "ax[1].set_xlabel(\"Epochs\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-08-04T06:27:37.668875Z",
     "iopub.status.idle": "2022-08-04T06:27:37.670015Z"
    }
   },
   "outputs": [],
   "source": [
    "# predicting on test data.\n",
    "pred_test = model.predict(x_test)\n",
    "y_pred = encoder.inverse_transform(pred_test)\n",
    "\n",
    "y_test = encoder.inverse_transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-08-04T06:27:37.671917Z",
     "iopub.status.idle": "2022-08-04T06:27:37.673098Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=['Predicted Labels', 'Actual Labels'])\n",
    "df['Predicted Labels'] = y_pred.flatten()\n",
    "df['Actual Labels'] = y_test.flatten()\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-08-04T06:27:37.674872Z",
     "iopub.status.idle": "2022-08-04T06:27:37.675966Z"
    }
   },
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize = (12, 10))\n",
    "cm = pd.DataFrame(cm , index = [i for i in encoder.categories_] , columns = [i for i in encoder.categories_])\n",
    "sns.heatmap(cm, linecolor='white', cmap='Blues', linewidth=1, annot=True, fmt='')\n",
    "plt.title('Confusion Matrix', size=20)\n",
    "plt.xlabel('Predicted Labels', size=14)\n",
    "plt.ylabel('Actual Labels', size=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-08-04T06:27:37.677738Z",
     "iopub.status.idle": "2022-08-04T06:27:37.678873Z"
    }
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can see our model is more accurate in predicting surprise, angry emotions and it makes sense also because audio files of these emotions differ to other audio files in a lot of ways like pitch, speed etc..\n",
    "- We overall achieved 61% accuracy on our test data and its decent but we can improve it more by applying more augmentation techniques and using other feature extraction methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is all i wanna do in this project. Hope you guyz like this. \n",
    "### If you like the kernel make sure to upvote it please :-)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
