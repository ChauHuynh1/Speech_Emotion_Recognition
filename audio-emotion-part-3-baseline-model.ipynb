{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Audio Emotion Recognition</center>\n",
    "## <center>Part 3 - Baseline model</center>\n",
    "#### <center> 24th August 2019 </center> \n",
    "#####  <center> Eu Jin Lok </center> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction \n",
    "Continuing where we left off in [Part 1](https://www.kaggle.com/ejlok1/audio-emotion-recognition-part-1-explore-data) and [Part 2](https://www.kaggle.com/ejlok1/audio-emotion-recognition-part-2-feature-extra), here we'll build a baseline model for an emotion classifier. When I say baseline, I mean its the simplest most parsimonious model I can think of. And view points will vary from one data scientist to another, but essentially its a model __NOT__ meant to achieve full accuracy potential. It's just to qucikly test the framework and setup the blueprint for how we go about creating a workable emotion classifier, cause at the moment, we don't know what works and what doesn't. This is a long notebook so this is the agenda below: \n",
    "\n",
    "1. [Data preparation and processing](#data)\n",
    "    * [Data preparation](#preparation)\n",
    "    * [Data processing](#processing)\n",
    "2. [Modelling](#modelling)\n",
    "3. [Model serialisation](#serialise)\n",
    "4. [Model validation](#validation)\n",
    "5. [Final thoughts](#final)\n",
    "\n",
    "Upvote this notebook if you like, and be sure to check out the other parts which are now available:\n",
    "* [Part 4 | Apply to new audio data](https://www.kaggle.com/ejlok1/audio-emotion-part-4-apply-to-new-audio-data)\n",
    "* [Part 5 | Data augmentation](https://www.kaggle.com/ejlok1/audio-emotion-part-5-data-augmentation)\n",
    "\n",
    "Most importantly, I want to thank the 4 authors for their excellent dataset, without it, writing this notebook could not have been possible. The original source of the dataset links are below:\n",
    "\n",
    "- [TESS](https://tspace.library.utoronto.ca/handle/1807/24487)\n",
    "- [CREMA-D](https://github.com/CheyneyComputerScience/CREMA-D)\n",
    "- [SAVEE](http://kahlan.eps.surrey.ac.uk/savee/Database.html)\n",
    "- [RAVDESS](https://zenodo.org/record/1188976#.XYP8CSgzaUk)\n",
    "- [RAVDESS_Kaggle](https://www.kaggle.com/uwrfkaggler/ravdess-emotional-speech-audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "# Importing required libraries \n",
    "# Keras\n",
    "import keras\n",
    "from keras import regularizers\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential, Model, model_from_json\n",
    "from keras.layers import Dense, Embedding, LSTM\n",
    "from keras.layers import Input, Flatten, Dropout, Activation, BatchNormalization\n",
    "from keras.layers import Conv1D, MaxPooling1D, AveragePooling1D\n",
    "from keras.utils import np_utils, to_categorical\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# sklearn\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Other  \n",
    "import librosa\n",
    "import librosa.display\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from matplotlib.pyplot import specgram\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import glob \n",
    "import os\n",
    "import pickle\n",
    "import IPython.display as ipd  # To play sound in the notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"data\"></a>\n",
    "## 1. Data preparation and processing\n",
    "We saw in [Part 1](https://www.kaggle.com/ejlok1/audio-emotion-recognition-part-1-explore-data) and [Part 2](https://www.kaggle.com/ejlok1/audio-emotion-recognition-part-2-feature-extra) the way we process the audio file into data and the MFCC features we extracted. We're going to do the same thing here except we process the entirity of the audio files. First up we need the reference file that contains the path to the raw audio files for training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"preparation\"></a>\n",
    "### Data preparation\n",
    "Lets pick up the meta-data file which we save in [part 1](\"https://www.kaggle.com/ejlok1/audio-emotion-recognition-part-1-explore-data\"), we're going to need it here to run a loop over it to read all the audio files spread across the 4 directories. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>source</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>male_sad</td>\n",
       "      <td>SAVEE</td>\n",
       "      <td>savee/JK_sa01.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>male_sad</td>\n",
       "      <td>SAVEE</td>\n",
       "      <td>savee/JK_sa15.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>male_neutral</td>\n",
       "      <td>SAVEE</td>\n",
       "      <td>savee/DC_n13.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>male_surprise</td>\n",
       "      <td>SAVEE</td>\n",
       "      <td>savee/DC_su09.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>male_neutral</td>\n",
       "      <td>SAVEE</td>\n",
       "      <td>savee/DC_n07.wav</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          labels source               path\n",
       "0       male_sad  SAVEE  savee/JK_sa01.wav\n",
       "1       male_sad  SAVEE  savee/JK_sa15.wav\n",
       "2   male_neutral  SAVEE   savee/DC_n13.wav\n",
       "3  male_surprise  SAVEE  savee/DC_su09.wav\n",
       "4   male_neutral  SAVEE   savee/DC_n07.wav"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets pick up the meta-data that we got from our first part of the Kernel\n",
    "ref = pd.read_csv(\"Data_path.csv\")\n",
    "ref.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we've already seen the shape of an MFCC output for each file, and it's a 2D matrix of the number of bands by time. In order to optimise space and memory, we're going to read each audio file, extract its mean across all MFCC bands by time, and  just keep the extracted features, dropping the entire audio file data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12162\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[-8.812323, -12.111704, -22.594236, -21.481213...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[-8.846449, -13.126469, -24.391258, -23.972637...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[-2.0016844, -2.284187, -8.079335, -7.4936047,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[-3.6122105, -4.3207064, -7.5397205, -8.864785...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[-0.89232916, -2.7065623, -9.859098, -8.692251...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             feature\n",
       "0  [-8.812323, -12.111704, -22.594236, -21.481213...\n",
       "1  [-8.846449, -13.126469, -24.391258, -23.972637...\n",
       "2  [-2.0016844, -2.284187, -8.079335, -7.4936047,...\n",
       "3  [-3.6122105, -4.3207064, -7.5397205, -8.864785...\n",
       "4  [-0.89232916, -2.7065623, -9.859098, -8.692251..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note this takes a couple of minutes (~10 mins) as we're iterating over 4 datasets \n",
    "df = pd.DataFrame(columns=['feature'])\n",
    "\n",
    "# loop feature extraction over the entire dataset\n",
    "counter=0\n",
    "for index,path in enumerate(ref.path):\n",
    "    X, sample_rate = librosa.load(path\n",
    "                                  , res_type='kaiser_fast'\n",
    "                                  ,duration=2.5\n",
    "                                  ,sr=44100\n",
    "                                  ,offset=0.5\n",
    "                                 )\n",
    "    sample_rate = np.array(sample_rate)\n",
    "    \n",
    "    # mean as the feature. Could do min and max etc as well. \n",
    "    mfccs = np.mean(librosa.feature.mfcc(y=X, \n",
    "                                        sr=sample_rate, \n",
    "                                        n_mfcc=13),\n",
    "                    axis=0)\n",
    "    df.loc[counter] = [mfccs]\n",
    "    counter=counter+1   \n",
    "\n",
    "# Check a few records to make sure its processed successfully\n",
    "print(len(df))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"processing\"></a>\n",
    "### Data processing\n",
    "\n",
    "Like any good standard data science workflow, data processing is the most important step. Cause garbage in grabage out. So lets start munging the data into a workable format and pad out any issues we find. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>source</th>\n",
       "      <th>path</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>...</th>\n",
       "      <th>206</th>\n",
       "      <th>207</th>\n",
       "      <th>208</th>\n",
       "      <th>209</th>\n",
       "      <th>210</th>\n",
       "      <th>211</th>\n",
       "      <th>212</th>\n",
       "      <th>213</th>\n",
       "      <th>214</th>\n",
       "      <th>215</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>male_sad</td>\n",
       "      <td>SAVEE</td>\n",
       "      <td>savee/JK_sa01.wav</td>\n",
       "      <td>-8.812323</td>\n",
       "      <td>-12.111704</td>\n",
       "      <td>-22.594236</td>\n",
       "      <td>-21.481213</td>\n",
       "      <td>-20.949923</td>\n",
       "      <td>-20.414589</td>\n",
       "      <td>-20.267546</td>\n",
       "      <td>...</td>\n",
       "      <td>-8.499665</td>\n",
       "      <td>-10.080904</td>\n",
       "      <td>-12.700767</td>\n",
       "      <td>-17.040066</td>\n",
       "      <td>-20.240370</td>\n",
       "      <td>-23.302593</td>\n",
       "      <td>-24.621033</td>\n",
       "      <td>-23.829395</td>\n",
       "      <td>-14.862103</td>\n",
       "      <td>-9.119078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>male_sad</td>\n",
       "      <td>SAVEE</td>\n",
       "      <td>savee/JK_sa15.wav</td>\n",
       "      <td>-8.846449</td>\n",
       "      <td>-13.126469</td>\n",
       "      <td>-24.391258</td>\n",
       "      <td>-23.972637</td>\n",
       "      <td>-23.494141</td>\n",
       "      <td>-24.208843</td>\n",
       "      <td>-25.631187</td>\n",
       "      <td>...</td>\n",
       "      <td>-8.738686</td>\n",
       "      <td>-8.822194</td>\n",
       "      <td>-8.977810</td>\n",
       "      <td>-9.998902</td>\n",
       "      <td>-15.777991</td>\n",
       "      <td>-22.670012</td>\n",
       "      <td>-23.585888</td>\n",
       "      <td>-24.138813</td>\n",
       "      <td>-17.324165</td>\n",
       "      <td>-9.067064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>male_neutral</td>\n",
       "      <td>SAVEE</td>\n",
       "      <td>savee/DC_n13.wav</td>\n",
       "      <td>-2.001684</td>\n",
       "      <td>-2.284187</td>\n",
       "      <td>-8.079335</td>\n",
       "      <td>-7.493605</td>\n",
       "      <td>-7.611511</td>\n",
       "      <td>-5.591491</td>\n",
       "      <td>-4.388685</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>male_surprise</td>\n",
       "      <td>SAVEE</td>\n",
       "      <td>savee/DC_su09.wav</td>\n",
       "      <td>-3.612211</td>\n",
       "      <td>-4.320706</td>\n",
       "      <td>-7.539721</td>\n",
       "      <td>-8.864785</td>\n",
       "      <td>-8.661814</td>\n",
       "      <td>-8.826547</td>\n",
       "      <td>-9.143905</td>\n",
       "      <td>...</td>\n",
       "      <td>-25.902903</td>\n",
       "      <td>-25.006645</td>\n",
       "      <td>-24.709745</td>\n",
       "      <td>-25.516710</td>\n",
       "      <td>-26.941380</td>\n",
       "      <td>-25.354641</td>\n",
       "      <td>-25.213074</td>\n",
       "      <td>-27.607460</td>\n",
       "      <td>-16.149431</td>\n",
       "      <td>-8.528477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>male_neutral</td>\n",
       "      <td>SAVEE</td>\n",
       "      <td>savee/DC_n07.wav</td>\n",
       "      <td>-0.892329</td>\n",
       "      <td>-2.706562</td>\n",
       "      <td>-9.859098</td>\n",
       "      <td>-8.692251</td>\n",
       "      <td>-8.685308</td>\n",
       "      <td>-8.844448</td>\n",
       "      <td>-8.032232</td>\n",
       "      <td>...</td>\n",
       "      <td>-11.438392</td>\n",
       "      <td>-11.691320</td>\n",
       "      <td>-11.480921</td>\n",
       "      <td>-10.730117</td>\n",
       "      <td>-9.891497</td>\n",
       "      <td>-9.329517</td>\n",
       "      <td>-8.907434</td>\n",
       "      <td>-8.881425</td>\n",
       "      <td>-8.354048</td>\n",
       "      <td>-5.121765</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 219 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          labels source               path         0          1          2  \\\n",
       "0       male_sad  SAVEE  savee/JK_sa01.wav -8.812323 -12.111704 -22.594236   \n",
       "1       male_sad  SAVEE  savee/JK_sa15.wav -8.846449 -13.126469 -24.391258   \n",
       "2   male_neutral  SAVEE   savee/DC_n13.wav -2.001684  -2.284187  -8.079335   \n",
       "3  male_surprise  SAVEE  savee/DC_su09.wav -3.612211  -4.320706  -7.539721   \n",
       "4   male_neutral  SAVEE   savee/DC_n07.wav -0.892329  -2.706562  -9.859098   \n",
       "\n",
       "           3          4          5          6  ...        206        207  \\\n",
       "0 -21.481213 -20.949923 -20.414589 -20.267546  ...  -8.499665 -10.080904   \n",
       "1 -23.972637 -23.494141 -24.208843 -25.631187  ...  -8.738686  -8.822194   \n",
       "2  -7.493605  -7.611511  -5.591491  -4.388685  ...        NaN        NaN   \n",
       "3  -8.864785  -8.661814  -8.826547  -9.143905  ... -25.902903 -25.006645   \n",
       "4  -8.692251  -8.685308  -8.844448  -8.032232  ... -11.438392 -11.691320   \n",
       "\n",
       "         208        209        210        211        212        213  \\\n",
       "0 -12.700767 -17.040066 -20.240370 -23.302593 -24.621033 -23.829395   \n",
       "1  -8.977810  -9.998902 -15.777991 -22.670012 -23.585888 -24.138813   \n",
       "2        NaN        NaN        NaN        NaN        NaN        NaN   \n",
       "3 -24.709745 -25.516710 -26.941380 -25.354641 -25.213074 -27.607460   \n",
       "4 -11.480921 -10.730117  -9.891497  -9.329517  -8.907434  -8.881425   \n",
       "\n",
       "         214       215  \n",
       "0 -14.862103 -9.119078  \n",
       "1 -17.324165 -9.067064  \n",
       "2        NaN       NaN  \n",
       "3 -16.149431 -8.528477  \n",
       "4  -8.354048 -5.121765  \n",
       "\n",
       "[5 rows x 219 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now extract the mean bands to its own feature columns\n",
    "df = pd.concat([ref,pd.DataFrame(df['feature'].values.tolist())],axis=1)\n",
    "df[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12162, 219)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>source</th>\n",
       "      <th>path</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>...</th>\n",
       "      <th>206</th>\n",
       "      <th>207</th>\n",
       "      <th>208</th>\n",
       "      <th>209</th>\n",
       "      <th>210</th>\n",
       "      <th>211</th>\n",
       "      <th>212</th>\n",
       "      <th>213</th>\n",
       "      <th>214</th>\n",
       "      <th>215</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>male_sad</td>\n",
       "      <td>SAVEE</td>\n",
       "      <td>savee/JK_sa01.wav</td>\n",
       "      <td>-8.812323</td>\n",
       "      <td>-12.111704</td>\n",
       "      <td>-22.594236</td>\n",
       "      <td>-21.481213</td>\n",
       "      <td>-20.949923</td>\n",
       "      <td>-20.414589</td>\n",
       "      <td>-20.267546</td>\n",
       "      <td>...</td>\n",
       "      <td>-8.499665</td>\n",
       "      <td>-10.080904</td>\n",
       "      <td>-12.700767</td>\n",
       "      <td>-17.040066</td>\n",
       "      <td>-20.240370</td>\n",
       "      <td>-23.302593</td>\n",
       "      <td>-24.621033</td>\n",
       "      <td>-23.829395</td>\n",
       "      <td>-14.862103</td>\n",
       "      <td>-9.119078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>male_sad</td>\n",
       "      <td>SAVEE</td>\n",
       "      <td>savee/JK_sa15.wav</td>\n",
       "      <td>-8.846449</td>\n",
       "      <td>-13.126469</td>\n",
       "      <td>-24.391258</td>\n",
       "      <td>-23.972637</td>\n",
       "      <td>-23.494141</td>\n",
       "      <td>-24.208843</td>\n",
       "      <td>-25.631187</td>\n",
       "      <td>...</td>\n",
       "      <td>-8.738686</td>\n",
       "      <td>-8.822194</td>\n",
       "      <td>-8.977810</td>\n",
       "      <td>-9.998902</td>\n",
       "      <td>-15.777991</td>\n",
       "      <td>-22.670012</td>\n",
       "      <td>-23.585888</td>\n",
       "      <td>-24.138813</td>\n",
       "      <td>-17.324165</td>\n",
       "      <td>-9.067064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>male_neutral</td>\n",
       "      <td>SAVEE</td>\n",
       "      <td>savee/DC_n13.wav</td>\n",
       "      <td>-2.001684</td>\n",
       "      <td>-2.284187</td>\n",
       "      <td>-8.079335</td>\n",
       "      <td>-7.493605</td>\n",
       "      <td>-7.611511</td>\n",
       "      <td>-5.591491</td>\n",
       "      <td>-4.388685</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>male_surprise</td>\n",
       "      <td>SAVEE</td>\n",
       "      <td>savee/DC_su09.wav</td>\n",
       "      <td>-3.612211</td>\n",
       "      <td>-4.320706</td>\n",
       "      <td>-7.539721</td>\n",
       "      <td>-8.864785</td>\n",
       "      <td>-8.661814</td>\n",
       "      <td>-8.826547</td>\n",
       "      <td>-9.143905</td>\n",
       "      <td>...</td>\n",
       "      <td>-25.902903</td>\n",
       "      <td>-25.006645</td>\n",
       "      <td>-24.709745</td>\n",
       "      <td>-25.516710</td>\n",
       "      <td>-26.941380</td>\n",
       "      <td>-25.354641</td>\n",
       "      <td>-25.213074</td>\n",
       "      <td>-27.607460</td>\n",
       "      <td>-16.149431</td>\n",
       "      <td>-8.528477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>male_neutral</td>\n",
       "      <td>SAVEE</td>\n",
       "      <td>savee/DC_n07.wav</td>\n",
       "      <td>-0.892329</td>\n",
       "      <td>-2.706562</td>\n",
       "      <td>-9.859098</td>\n",
       "      <td>-8.692251</td>\n",
       "      <td>-8.685308</td>\n",
       "      <td>-8.844448</td>\n",
       "      <td>-8.032232</td>\n",
       "      <td>...</td>\n",
       "      <td>-11.438392</td>\n",
       "      <td>-11.691320</td>\n",
       "      <td>-11.480921</td>\n",
       "      <td>-10.730117</td>\n",
       "      <td>-9.891497</td>\n",
       "      <td>-9.329517</td>\n",
       "      <td>-8.907434</td>\n",
       "      <td>-8.881425</td>\n",
       "      <td>-8.354048</td>\n",
       "      <td>-5.121765</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 219 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          labels source               path         0          1          2  \\\n",
       "0       male_sad  SAVEE  savee/JK_sa01.wav -8.812323 -12.111704 -22.594236   \n",
       "1       male_sad  SAVEE  savee/JK_sa15.wav -8.846449 -13.126469 -24.391258   \n",
       "2   male_neutral  SAVEE   savee/DC_n13.wav -2.001684  -2.284187  -8.079335   \n",
       "3  male_surprise  SAVEE  savee/DC_su09.wav -3.612211  -4.320706  -7.539721   \n",
       "4   male_neutral  SAVEE   savee/DC_n07.wav -0.892329  -2.706562  -9.859098   \n",
       "\n",
       "           3          4          5          6  ...        206        207  \\\n",
       "0 -21.481213 -20.949923 -20.414589 -20.267546  ...  -8.499665 -10.080904   \n",
       "1 -23.972637 -23.494141 -24.208843 -25.631187  ...  -8.738686  -8.822194   \n",
       "2  -7.493605  -7.611511  -5.591491  -4.388685  ...   0.000000   0.000000   \n",
       "3  -8.864785  -8.661814  -8.826547  -9.143905  ... -25.902903 -25.006645   \n",
       "4  -8.692251  -8.685308  -8.844448  -8.032232  ... -11.438392 -11.691320   \n",
       "\n",
       "         208        209        210        211        212        213  \\\n",
       "0 -12.700767 -17.040066 -20.240370 -23.302593 -24.621033 -23.829395   \n",
       "1  -8.977810  -9.998902 -15.777991 -22.670012 -23.585888 -24.138813   \n",
       "2   0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "3 -24.709745 -25.516710 -26.941380 -25.354641 -25.213074 -27.607460   \n",
       "4 -11.480921 -10.730117  -9.891497  -9.329517  -8.907434  -8.881425   \n",
       "\n",
       "         214       215  \n",
       "0 -14.862103 -9.119078  \n",
       "1 -17.324165 -9.067064  \n",
       "2   0.000000  0.000000  \n",
       "3 -16.149431 -8.528477  \n",
       "4  -8.354048 -5.121765  \n",
       "\n",
       "[5 rows x 219 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# replace NA with 0\n",
    "df=df.fillna(0)\n",
    "print(df.shape)\n",
    "df[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that looks alot better. Next step we will split the data into 2 parts, one for training and one for validation. This ensures we measure the model's performance at its true accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>206</th>\n",
       "      <th>207</th>\n",
       "      <th>208</th>\n",
       "      <th>209</th>\n",
       "      <th>210</th>\n",
       "      <th>211</th>\n",
       "      <th>212</th>\n",
       "      <th>213</th>\n",
       "      <th>214</th>\n",
       "      <th>215</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4950</th>\n",
       "      <td>-17.142696</td>\n",
       "      <td>-17.249537</td>\n",
       "      <td>-18.365580</td>\n",
       "      <td>-18.948351</td>\n",
       "      <td>-17.365459</td>\n",
       "      <td>-16.711090</td>\n",
       "      <td>-17.699482</td>\n",
       "      <td>-18.021383</td>\n",
       "      <td>-17.897398</td>\n",
       "      <td>-15.878500</td>\n",
       "      <td>...</td>\n",
       "      <td>-22.865425</td>\n",
       "      <td>-21.614162</td>\n",
       "      <td>-19.724928</td>\n",
       "      <td>-18.845333</td>\n",
       "      <td>-19.363422</td>\n",
       "      <td>-20.13763</td>\n",
       "      <td>-22.655140</td>\n",
       "      <td>-24.578310</td>\n",
       "      <td>-24.039164</td>\n",
       "      <td>-23.209587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3860</th>\n",
       "      <td>-18.354397</td>\n",
       "      <td>-20.836168</td>\n",
       "      <td>-22.571033</td>\n",
       "      <td>-22.155430</td>\n",
       "      <td>-20.623474</td>\n",
       "      <td>-17.927940</td>\n",
       "      <td>-15.832611</td>\n",
       "      <td>-18.623466</td>\n",
       "      <td>-21.543407</td>\n",
       "      <td>-24.989614</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9761</th>\n",
       "      <td>-4.823563</td>\n",
       "      <td>-6.056048</td>\n",
       "      <td>-9.580624</td>\n",
       "      <td>-12.012063</td>\n",
       "      <td>-9.959867</td>\n",
       "      <td>-11.912548</td>\n",
       "      <td>-13.994515</td>\n",
       "      <td>-13.555813</td>\n",
       "      <td>-14.022306</td>\n",
       "      <td>-15.118246</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7620</th>\n",
       "      <td>-7.031147</td>\n",
       "      <td>-4.253550</td>\n",
       "      <td>-4.534487</td>\n",
       "      <td>-5.836689</td>\n",
       "      <td>-5.248199</td>\n",
       "      <td>-6.456452</td>\n",
       "      <td>-8.122453</td>\n",
       "      <td>-9.154642</td>\n",
       "      <td>-8.647813</td>\n",
       "      <td>-8.178625</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11586</th>\n",
       "      <td>-22.565975</td>\n",
       "      <td>-21.767015</td>\n",
       "      <td>-20.529488</td>\n",
       "      <td>-20.669310</td>\n",
       "      <td>-21.171085</td>\n",
       "      <td>-18.573402</td>\n",
       "      <td>-18.412350</td>\n",
       "      <td>-16.178038</td>\n",
       "      <td>-14.222462</td>\n",
       "      <td>-15.122541</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7914</th>\n",
       "      <td>-20.082027</td>\n",
       "      <td>-18.982424</td>\n",
       "      <td>-17.009441</td>\n",
       "      <td>-16.944054</td>\n",
       "      <td>-19.334976</td>\n",
       "      <td>-19.527683</td>\n",
       "      <td>-21.974346</td>\n",
       "      <td>-20.144060</td>\n",
       "      <td>-17.954927</td>\n",
       "      <td>-19.302568</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9513</th>\n",
       "      <td>-20.103537</td>\n",
       "      <td>-18.625866</td>\n",
       "      <td>-16.116106</td>\n",
       "      <td>-16.929592</td>\n",
       "      <td>-18.197668</td>\n",
       "      <td>-18.149662</td>\n",
       "      <td>-19.240425</td>\n",
       "      <td>-18.361637</td>\n",
       "      <td>-16.917980</td>\n",
       "      <td>-16.639193</td>\n",
       "      <td>...</td>\n",
       "      <td>-17.848896</td>\n",
       "      <td>-19.357054</td>\n",
       "      <td>-17.748695</td>\n",
       "      <td>-19.136810</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5835</th>\n",
       "      <td>-21.078182</td>\n",
       "      <td>-18.671946</td>\n",
       "      <td>-17.676804</td>\n",
       "      <td>-18.009502</td>\n",
       "      <td>-18.218781</td>\n",
       "      <td>-18.676929</td>\n",
       "      <td>-16.592766</td>\n",
       "      <td>-17.796528</td>\n",
       "      <td>-17.992470</td>\n",
       "      <td>-17.824917</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5389</th>\n",
       "      <td>-22.707441</td>\n",
       "      <td>-20.160748</td>\n",
       "      <td>-18.926155</td>\n",
       "      <td>-19.429979</td>\n",
       "      <td>-19.278116</td>\n",
       "      <td>-18.125767</td>\n",
       "      <td>-18.068565</td>\n",
       "      <td>-19.879316</td>\n",
       "      <td>-20.896755</td>\n",
       "      <td>-19.176098</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11222</th>\n",
       "      <td>-20.008287</td>\n",
       "      <td>-17.640305</td>\n",
       "      <td>-19.188616</td>\n",
       "      <td>-18.435751</td>\n",
       "      <td>-17.538836</td>\n",
       "      <td>-17.817177</td>\n",
       "      <td>-17.898476</td>\n",
       "      <td>-20.555359</td>\n",
       "      <td>-18.578115</td>\n",
       "      <td>-15.957151</td>\n",
       "      <td>...</td>\n",
       "      <td>-18.031599</td>\n",
       "      <td>-18.101122</td>\n",
       "      <td>-19.630695</td>\n",
       "      <td>-20.707493</td>\n",
       "      <td>-20.239124</td>\n",
       "      <td>-18.33766</td>\n",
       "      <td>-15.543308</td>\n",
       "      <td>-17.478794</td>\n",
       "      <td>-17.017057</td>\n",
       "      <td>-19.112486</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 216 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0          1          2          3          4          5    \\\n",
       "4950  -17.142696 -17.249537 -18.365580 -18.948351 -17.365459 -16.711090   \n",
       "3860  -18.354397 -20.836168 -22.571033 -22.155430 -20.623474 -17.927940   \n",
       "9761   -4.823563  -6.056048  -9.580624 -12.012063  -9.959867 -11.912548   \n",
       "7620   -7.031147  -4.253550  -4.534487  -5.836689  -5.248199  -6.456452   \n",
       "11586 -22.565975 -21.767015 -20.529488 -20.669310 -21.171085 -18.573402   \n",
       "7914  -20.082027 -18.982424 -17.009441 -16.944054 -19.334976 -19.527683   \n",
       "9513  -20.103537 -18.625866 -16.116106 -16.929592 -18.197668 -18.149662   \n",
       "5835  -21.078182 -18.671946 -17.676804 -18.009502 -18.218781 -18.676929   \n",
       "5389  -22.707441 -20.160748 -18.926155 -19.429979 -19.278116 -18.125767   \n",
       "11222 -20.008287 -17.640305 -19.188616 -18.435751 -17.538836 -17.817177   \n",
       "\n",
       "             6          7          8          9    ...        206        207  \\\n",
       "4950  -17.699482 -18.021383 -17.897398 -15.878500  ... -22.865425 -21.614162   \n",
       "3860  -15.832611 -18.623466 -21.543407 -24.989614  ...   0.000000   0.000000   \n",
       "9761  -13.994515 -13.555813 -14.022306 -15.118246  ...   0.000000   0.000000   \n",
       "7620   -8.122453  -9.154642  -8.647813  -8.178625  ...   0.000000   0.000000   \n",
       "11586 -18.412350 -16.178038 -14.222462 -15.122541  ...   0.000000   0.000000   \n",
       "7914  -21.974346 -20.144060 -17.954927 -19.302568  ...   0.000000   0.000000   \n",
       "9513  -19.240425 -18.361637 -16.917980 -16.639193  ... -17.848896 -19.357054   \n",
       "5835  -16.592766 -17.796528 -17.992470 -17.824917  ...   0.000000   0.000000   \n",
       "5389  -18.068565 -19.879316 -20.896755 -19.176098  ...   0.000000   0.000000   \n",
       "11222 -17.898476 -20.555359 -18.578115 -15.957151  ... -18.031599 -18.101122   \n",
       "\n",
       "             208        209        210       211        212        213  \\\n",
       "4950  -19.724928 -18.845333 -19.363422 -20.13763 -22.655140 -24.578310   \n",
       "3860    0.000000   0.000000   0.000000   0.00000   0.000000   0.000000   \n",
       "9761    0.000000   0.000000   0.000000   0.00000   0.000000   0.000000   \n",
       "7620    0.000000   0.000000   0.000000   0.00000   0.000000   0.000000   \n",
       "11586   0.000000   0.000000   0.000000   0.00000   0.000000   0.000000   \n",
       "7914    0.000000   0.000000   0.000000   0.00000   0.000000   0.000000   \n",
       "9513  -17.748695 -19.136810   0.000000   0.00000   0.000000   0.000000   \n",
       "5835    0.000000   0.000000   0.000000   0.00000   0.000000   0.000000   \n",
       "5389    0.000000   0.000000   0.000000   0.00000   0.000000   0.000000   \n",
       "11222 -19.630695 -20.707493 -20.239124 -18.33766 -15.543308 -17.478794   \n",
       "\n",
       "             214        215  \n",
       "4950  -24.039164 -23.209587  \n",
       "3860    0.000000   0.000000  \n",
       "9761    0.000000   0.000000  \n",
       "7620    0.000000   0.000000  \n",
       "11586   0.000000   0.000000  \n",
       "7914    0.000000   0.000000  \n",
       "9513    0.000000   0.000000  \n",
       "5835    0.000000   0.000000  \n",
       "5389    0.000000   0.000000  \n",
       "11222 -17.017057 -19.112486  \n",
       "\n",
       "[10 rows x 216 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split between train and test \n",
    "X_train, X_test, y_train, y_test = train_test_split(df.drop(['path','labels','source'],axis=1)\n",
    "                                                    , df.labels\n",
    "                                                    , test_size=0.25\n",
    "                                                    , shuffle=True\n",
    "                                                    , random_state=42\n",
    "                                                   )\n",
    "\n",
    "# Lets see how the data present itself before normalisation \n",
    "X_train[150:160]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now because we are mixing up a few different data sources, it would be wise to normalise the data. This is proven to improve the accuracy and speed up the training process. Prior to the discovery of this solution in the embrionic years of neural network, the problem used to be know as \"exploding gradients\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>206</th>\n",
       "      <th>207</th>\n",
       "      <th>208</th>\n",
       "      <th>209</th>\n",
       "      <th>210</th>\n",
       "      <th>211</th>\n",
       "      <th>212</th>\n",
       "      <th>213</th>\n",
       "      <th>214</th>\n",
       "      <th>215</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4950</th>\n",
       "      <td>0.373270</td>\n",
       "      <td>0.352227</td>\n",
       "      <td>0.438397</td>\n",
       "      <td>0.389021</td>\n",
       "      <td>0.498961</td>\n",
       "      <td>0.542144</td>\n",
       "      <td>0.461018</td>\n",
       "      <td>0.432419</td>\n",
       "      <td>0.435712</td>\n",
       "      <td>0.581127</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.874217</td>\n",
       "      <td>-0.805569</td>\n",
       "      <td>-0.685764</td>\n",
       "      <td>-0.625542</td>\n",
       "      <td>-0.671389</td>\n",
       "      <td>-0.713683</td>\n",
       "      <td>-0.856419</td>\n",
       "      <td>-0.980170</td>\n",
       "      <td>-0.965940</td>\n",
       "      <td>-0.910889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3860</th>\n",
       "      <td>0.286191</td>\n",
       "      <td>0.088576</td>\n",
       "      <td>0.127417</td>\n",
       "      <td>0.152359</td>\n",
       "      <td>0.258929</td>\n",
       "      <td>0.452538</td>\n",
       "      <td>0.598172</td>\n",
       "      <td>0.388228</td>\n",
       "      <td>0.168150</td>\n",
       "      <td>-0.086836</td>\n",
       "      <td>...</td>\n",
       "      <td>0.539838</td>\n",
       "      <td>0.525971</td>\n",
       "      <td>0.525370</td>\n",
       "      <td>0.526296</td>\n",
       "      <td>0.511449</td>\n",
       "      <td>0.511353</td>\n",
       "      <td>0.512023</td>\n",
       "      <td>0.500333</td>\n",
       "      <td>0.493808</td>\n",
       "      <td>0.487405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9761</th>\n",
       "      <td>1.258581</td>\n",
       "      <td>1.175052</td>\n",
       "      <td>1.088016</td>\n",
       "      <td>0.900874</td>\n",
       "      <td>1.044564</td>\n",
       "      <td>0.895499</td>\n",
       "      <td>0.733212</td>\n",
       "      <td>0.760179</td>\n",
       "      <td>0.720084</td>\n",
       "      <td>0.636864</td>\n",
       "      <td>...</td>\n",
       "      <td>0.539838</td>\n",
       "      <td>0.525971</td>\n",
       "      <td>0.525370</td>\n",
       "      <td>0.526296</td>\n",
       "      <td>0.511449</td>\n",
       "      <td>0.511353</td>\n",
       "      <td>0.512023</td>\n",
       "      <td>0.500333</td>\n",
       "      <td>0.493808</td>\n",
       "      <td>0.487405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7620</th>\n",
       "      <td>1.099933</td>\n",
       "      <td>1.307553</td>\n",
       "      <td>1.461161</td>\n",
       "      <td>1.356577</td>\n",
       "      <td>1.391693</td>\n",
       "      <td>1.297275</td>\n",
       "      <td>1.164616</td>\n",
       "      <td>1.083213</td>\n",
       "      <td>1.114490</td>\n",
       "      <td>1.145629</td>\n",
       "      <td>...</td>\n",
       "      <td>0.539838</td>\n",
       "      <td>0.525971</td>\n",
       "      <td>0.525370</td>\n",
       "      <td>0.526296</td>\n",
       "      <td>0.511449</td>\n",
       "      <td>0.511353</td>\n",
       "      <td>0.512023</td>\n",
       "      <td>0.500333</td>\n",
       "      <td>0.493808</td>\n",
       "      <td>0.487405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11586</th>\n",
       "      <td>-0.016472</td>\n",
       "      <td>0.020150</td>\n",
       "      <td>0.278383</td>\n",
       "      <td>0.262025</td>\n",
       "      <td>0.218584</td>\n",
       "      <td>0.405007</td>\n",
       "      <td>0.408646</td>\n",
       "      <td>0.567715</td>\n",
       "      <td>0.705396</td>\n",
       "      <td>0.636549</td>\n",
       "      <td>...</td>\n",
       "      <td>0.539838</td>\n",
       "      <td>0.525971</td>\n",
       "      <td>0.525370</td>\n",
       "      <td>0.526296</td>\n",
       "      <td>0.511449</td>\n",
       "      <td>0.511353</td>\n",
       "      <td>0.512023</td>\n",
       "      <td>0.500333</td>\n",
       "      <td>0.493808</td>\n",
       "      <td>0.487405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7914</th>\n",
       "      <td>0.162036</td>\n",
       "      <td>0.224843</td>\n",
       "      <td>0.538679</td>\n",
       "      <td>0.536925</td>\n",
       "      <td>0.353858</td>\n",
       "      <td>0.334736</td>\n",
       "      <td>0.146956</td>\n",
       "      <td>0.276621</td>\n",
       "      <td>0.431490</td>\n",
       "      <td>0.330098</td>\n",
       "      <td>...</td>\n",
       "      <td>0.539838</td>\n",
       "      <td>0.525971</td>\n",
       "      <td>0.525370</td>\n",
       "      <td>0.526296</td>\n",
       "      <td>0.511449</td>\n",
       "      <td>0.511353</td>\n",
       "      <td>0.512023</td>\n",
       "      <td>0.500333</td>\n",
       "      <td>0.493808</td>\n",
       "      <td>0.487405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9513</th>\n",
       "      <td>0.160490</td>\n",
       "      <td>0.251054</td>\n",
       "      <td>0.604738</td>\n",
       "      <td>0.537992</td>\n",
       "      <td>0.437648</td>\n",
       "      <td>0.436211</td>\n",
       "      <td>0.347810</td>\n",
       "      <td>0.407445</td>\n",
       "      <td>0.507586</td>\n",
       "      <td>0.525359</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.563982</td>\n",
       "      <td>-0.666520</td>\n",
       "      <td>-0.564421</td>\n",
       "      <td>-0.643357</td>\n",
       "      <td>0.511449</td>\n",
       "      <td>0.511353</td>\n",
       "      <td>0.512023</td>\n",
       "      <td>0.500333</td>\n",
       "      <td>0.493808</td>\n",
       "      <td>0.487405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5835</th>\n",
       "      <td>0.090447</td>\n",
       "      <td>0.247666</td>\n",
       "      <td>0.489329</td>\n",
       "      <td>0.458302</td>\n",
       "      <td>0.436093</td>\n",
       "      <td>0.397384</td>\n",
       "      <td>0.542326</td>\n",
       "      <td>0.448923</td>\n",
       "      <td>0.428735</td>\n",
       "      <td>0.438429</td>\n",
       "      <td>...</td>\n",
       "      <td>0.539838</td>\n",
       "      <td>0.525971</td>\n",
       "      <td>0.525370</td>\n",
       "      <td>0.526296</td>\n",
       "      <td>0.511449</td>\n",
       "      <td>0.511353</td>\n",
       "      <td>0.512023</td>\n",
       "      <td>0.500333</td>\n",
       "      <td>0.493808</td>\n",
       "      <td>0.487405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5389</th>\n",
       "      <td>-0.026639</td>\n",
       "      <td>0.138226</td>\n",
       "      <td>0.396944</td>\n",
       "      <td>0.353480</td>\n",
       "      <td>0.358047</td>\n",
       "      <td>0.437970</td>\n",
       "      <td>0.433903</td>\n",
       "      <td>0.296052</td>\n",
       "      <td>0.215605</td>\n",
       "      <td>0.339370</td>\n",
       "      <td>...</td>\n",
       "      <td>0.539838</td>\n",
       "      <td>0.525971</td>\n",
       "      <td>0.525370</td>\n",
       "      <td>0.526296</td>\n",
       "      <td>0.511449</td>\n",
       "      <td>0.511353</td>\n",
       "      <td>0.512023</td>\n",
       "      <td>0.500333</td>\n",
       "      <td>0.493808</td>\n",
       "      <td>0.487405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11222</th>\n",
       "      <td>0.167335</td>\n",
       "      <td>0.323502</td>\n",
       "      <td>0.377536</td>\n",
       "      <td>0.426847</td>\n",
       "      <td>0.486188</td>\n",
       "      <td>0.460694</td>\n",
       "      <td>0.446399</td>\n",
       "      <td>0.246432</td>\n",
       "      <td>0.385757</td>\n",
       "      <td>0.575361</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.575281</td>\n",
       "      <td>-0.589149</td>\n",
       "      <td>-0.679978</td>\n",
       "      <td>-0.739358</td>\n",
       "      <td>-0.724882</td>\n",
       "      <td>-0.604185</td>\n",
       "      <td>-0.426842</td>\n",
       "      <td>-0.552522</td>\n",
       "      <td>-0.539532</td>\n",
       "      <td>-0.664053</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 216 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6    \\\n",
       "4950   0.373270  0.352227  0.438397  0.389021  0.498961  0.542144  0.461018   \n",
       "3860   0.286191  0.088576  0.127417  0.152359  0.258929  0.452538  0.598172   \n",
       "9761   1.258581  1.175052  1.088016  0.900874  1.044564  0.895499  0.733212   \n",
       "7620   1.099933  1.307553  1.461161  1.356577  1.391693  1.297275  1.164616   \n",
       "11586 -0.016472  0.020150  0.278383  0.262025  0.218584  0.405007  0.408646   \n",
       "7914   0.162036  0.224843  0.538679  0.536925  0.353858  0.334736  0.146956   \n",
       "9513   0.160490  0.251054  0.604738  0.537992  0.437648  0.436211  0.347810   \n",
       "5835   0.090447  0.247666  0.489329  0.458302  0.436093  0.397384  0.542326   \n",
       "5389  -0.026639  0.138226  0.396944  0.353480  0.358047  0.437970  0.433903   \n",
       "11222  0.167335  0.323502  0.377536  0.426847  0.486188  0.460694  0.446399   \n",
       "\n",
       "            7         8         9    ...       206       207       208  \\\n",
       "4950   0.432419  0.435712  0.581127  ... -0.874217 -0.805569 -0.685764   \n",
       "3860   0.388228  0.168150 -0.086836  ...  0.539838  0.525971  0.525370   \n",
       "9761   0.760179  0.720084  0.636864  ...  0.539838  0.525971  0.525370   \n",
       "7620   1.083213  1.114490  1.145629  ...  0.539838  0.525971  0.525370   \n",
       "11586  0.567715  0.705396  0.636549  ...  0.539838  0.525971  0.525370   \n",
       "7914   0.276621  0.431490  0.330098  ...  0.539838  0.525971  0.525370   \n",
       "9513   0.407445  0.507586  0.525359  ... -0.563982 -0.666520 -0.564421   \n",
       "5835   0.448923  0.428735  0.438429  ...  0.539838  0.525971  0.525370   \n",
       "5389   0.296052  0.215605  0.339370  ...  0.539838  0.525971  0.525370   \n",
       "11222  0.246432  0.385757  0.575361  ... -0.575281 -0.589149 -0.679978   \n",
       "\n",
       "            209       210       211       212       213       214       215  \n",
       "4950  -0.625542 -0.671389 -0.713683 -0.856419 -0.980170 -0.965940 -0.910889  \n",
       "3860   0.526296  0.511449  0.511353  0.512023  0.500333  0.493808  0.487405  \n",
       "9761   0.526296  0.511449  0.511353  0.512023  0.500333  0.493808  0.487405  \n",
       "7620   0.526296  0.511449  0.511353  0.512023  0.500333  0.493808  0.487405  \n",
       "11586  0.526296  0.511449  0.511353  0.512023  0.500333  0.493808  0.487405  \n",
       "7914   0.526296  0.511449  0.511353  0.512023  0.500333  0.493808  0.487405  \n",
       "9513  -0.643357  0.511449  0.511353  0.512023  0.500333  0.493808  0.487405  \n",
       "5835   0.526296  0.511449  0.511353  0.512023  0.500333  0.493808  0.487405  \n",
       "5389   0.526296  0.511449  0.511353  0.512023  0.500333  0.493808  0.487405  \n",
       "11222 -0.739358 -0.724882 -0.604185 -0.426842 -0.552522 -0.539532 -0.664053  \n",
       "\n",
       "[10 rows x 216 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lts do data normalization \n",
    "mean = np.mean(X_train, axis=0)\n",
    "std = np.std(X_train, axis=0)\n",
    "\n",
    "X_train = (X_train - mean)/std\n",
    "X_test = (X_test - mean)/std\n",
    "\n",
    "# Check the dataset now \n",
    "X_train[150:160]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of documentation, I'll just mention here that there's another method for normalisation but it hasn't worked out well, at least not when I implemented it. So I swapped in for something more simple, which is what i've implemented above. Perhaps someone else could give it a try below\n",
    "\n",
    "```python\n",
    "max_data = np.max(X_train)\n",
    "min_data = np.min(X_train)\n",
    "X_train = (X_train-min_data)/(max_data-min_data+1e-6)\n",
    "X_train =  X_train-0.5\n",
    "\n",
    "max_data = np.max(X_test)\n",
    "min_data = np.min(X_test)\n",
    "X_test = (X_test-min_data)/(max_data-min_data+1e-6)\n",
    "X_test =  X_test-0.5\n",
    "\n",
    "X_train[150:160]\n",
    "```\n",
    "\n",
    "Next part we'll need to convert the data format to a numpy array, because we are using keras. Initially I had plans to use XGboost or LightGBM for this task. But since I've potential plans to move to a 2D CNN, it may make sense to continue on the Deep Learning path way and implement a ID CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9121, 216)\n",
      "['female_angry' 'female_disgust' 'female_fear' 'female_happy'\n",
      " 'female_neutral' 'female_sad' 'female_surprise' 'male_angry'\n",
      " 'male_disgust' 'male_fear' 'male_happy' 'male_neutral' 'male_sad'\n",
      " 'male_surprise']\n"
     ]
    }
   ],
   "source": [
    "# Lets few preparation steps to get it into the correct format for Keras \n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "# one hot encode the target \n",
    "lb = LabelEncoder()\n",
    "y_train = np_utils.to_categorical(lb.fit_transform(y_train))\n",
    "y_test = np_utils.to_categorical(lb.fit_transform(y_test))\n",
    "\n",
    "print(X_train.shape)\n",
    "print(lb.classes_)\n",
    "#print(y_train[0:10])\n",
    "#print(y_test[0:10])\n",
    "\n",
    "# Pickel the lb object for future use \n",
    "filename = 'labels'\n",
    "outfile = open(filename,'wb')\n",
    "pickle.dump(lb,outfile)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------\n",
    "Now because we are using a CNN, we need to specify the 3rd dimension, which for us is 1. Its 1 because we're doing a 1D CNN and not a 2D CNN. If we use the MFCC data in its entirity, we could feed that through as the input data, thus making the network a 2D CNN.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9121, 216, 1)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = np.expand_dims(X_train, axis=2)\n",
    "X_test = np.expand_dims(X_test, axis=2)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"modelling\"></a>\n",
    "## 2. Modelling\n",
    "The architecture of the model below is based on a few sources that I've seen before such as Kaggle and Stackoverflow. I'm unable to find the source but safe to say this particular format works quite well and is fast, although I've used GPU. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1 Pro\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-05 12:32:04.094321: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-10-05 12:32:04.094445: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'keras.optimizers' has no attribute 'rmsprop'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [10], line 30\u001b[0m\n\u001b[1;32m     27\u001b[0m model\u001b[38;5;241m.\u001b[39madd(Activation(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# opt = keras.optimizers.SGD(lr=0.0001, momentum=0.0, decay=0.0, nesterov=False)\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# opt = keras.optimizers.Adam(lr=0.0001)\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m opt \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mrmsprop(lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.00001\u001b[39m, decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-6\u001b[39m)\n\u001b[1;32m     31\u001b[0m model\u001b[38;5;241m.\u001b[39msummary()\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'keras.optimizers' has no attribute 'rmsprop'"
     ]
    }
   ],
   "source": [
    "# New model\n",
    "model = Sequential()\n",
    "model.add(Conv1D(256, 8, padding='same',input_shape=(X_train.shape[1],1)))  # X_train.shape[1] = No. of Columns\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv1D(256, 8, padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(MaxPooling1D(pool_size=(8)))\n",
    "model.add(Conv1D(128, 8, padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv1D(128, 8, padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv1D(128, 8, padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv1D(128, 8, padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(MaxPooling1D(pool_size=(8)))\n",
    "model.add(Conv1D(64, 8, padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv1D(64, 8, padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(14)) # Target class number\n",
    "model.add(Activation('softmax'))\n",
    "# opt = keras.optimizers.SGD(lr=0.0001, momentum=0.0, decay=0.0, nesterov=False)\n",
    "# opt = keras.optimizers.Adam(lr=0.0001)\n",
    "opt = keras.optimizers.rmsprop(lr=0.00001, decay=1e-6)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer=opt,metrics=['accuracy'])\n",
    "model_history=model.fit(X_train, y_train, batch_size=16, epochs=100, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(model_history.history['loss'])\n",
    "plt.plot(model_history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can see that the loss starts to plateau now at around 50 epochs. Regardless we'll keep it at 100 as the final model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"serialise\"></a>\n",
    "## 3. Model serialisation\n",
    "So its time to serialise the model for re-usability. Serialisation and saving mean the same thing. We need to serialise the model architecture and the weights, thats all. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model and weights\n",
    "model_name = 'Emotion_Model.h5'\n",
    "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
    "\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "model_path = os.path.join(save_dir, model_name)\n",
    "model.save(model_path)\n",
    "print('Save model and weights at %s ' % model_path)\n",
    "\n",
    "# Save the model to disk\n",
    "model_json = model.to_json()\n",
    "with open(\"model_json.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"validation\"></a>\n",
    "## 4. Model validation\n",
    "Now predicting emotions on the test data. After serialising the model above, i'm going to just reload it into disk. Essentially to re-use the model without having to retrain by re-running the code, we just need to run this section of the code and apply the model to a new dataset. Since we used the same test set in the keras model, the result is essentially the same as the last epoch of 100 which is 43.80%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading json and model architecture \n",
    "json_file = open('model_json.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"saved_models/Emotion_Model.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    " \n",
    "# Keras optimiser\n",
    "opt = keras.optimizers.rmsprop(lr=0.00001, decay=1e-6)\n",
    "loaded_model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "score = loaded_model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = loaded_model.predict(X_test, \n",
    "                         batch_size=16, \n",
    "                         verbose=1)\n",
    "\n",
    "preds=preds.argmax(axis=1)\n",
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prediction is in the form of numbers, we'll need to append the labels to it before we run the accuracy measure..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions \n",
    "preds = preds.astype(int).flatten()\n",
    "preds = (lb.inverse_transform((preds)))\n",
    "preds = pd.DataFrame({'predictedvalues': preds})\n",
    "\n",
    "# Actual labels\n",
    "actual=y_test.argmax(axis=1)\n",
    "actual = actual.astype(int).flatten()\n",
    "actual = (lb.inverse_transform((actual)))\n",
    "actual = pd.DataFrame({'actualvalues': actual})\n",
    "\n",
    "# Lets combined both of them into a single dataframe\n",
    "finaldf = actual.join(preds)\n",
    "finaldf[170:180]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets write the predictions out into a file for re-use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write out the predictions to disk\n",
    "finaldf.to_csv('Predictions.csv', index=False)\n",
    "finaldf.groupby('predictedvalues').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we've made our predictions, so how well have we done? We're going to use the most simplest form of accuracy measure which is absolute accuracy, which is really just the % of records where Actual = Predicted, over the total number of records predicted. We'll also produce the F1, recall and precision scores. \n",
    "\n",
    "The most common way to visualise this output is via a confusion matrix. I found an excellent heat map plot to visualise the accuracy of the confusion matrix [here](https://gist.github.com/shaypal5/94c53d765083101efc0240d776a23823) which i've borrowed for this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the confusion matrix heat map plot\n",
    "def print_confusion_matrix(confusion_matrix, class_names, figsize = (10,7), fontsize=14):\n",
    "    \"\"\"Prints a confusion matrix, as returned by sklearn.metrics.confusion_matrix, as a heatmap.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    confusion_matrix: numpy.ndarray\n",
    "        The numpy.ndarray object returned from a call to sklearn.metrics.confusion_matrix. \n",
    "        Similarly constructed ndarrays can also be used.\n",
    "    class_names: list\n",
    "        An ordered list of class names, in the order they index the given confusion matrix.\n",
    "    figsize: tuple\n",
    "        A 2-long tuple, the first value determining the horizontal size of the ouputted figure,\n",
    "        the second determining the vertical size. Defaults to (10,7).\n",
    "    fontsize: int\n",
    "        Font size for axes labels. Defaults to 14.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    matplotlib.figure.Figure\n",
    "        The resulting confusion matrix figure\n",
    "    \"\"\"\n",
    "    df_cm = pd.DataFrame(\n",
    "        confusion_matrix, index=class_names, columns=class_names, \n",
    "    )\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    try:\n",
    "        heatmap = sns.heatmap(df_cm, annot=True, fmt=\"d\")\n",
    "    except ValueError:\n",
    "        raise ValueError(\"Confusion matrix values must be integers.\")\n",
    "        \n",
    "    heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right', fontsize=fontsize)\n",
    "    heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45, ha='right', fontsize=fontsize)\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "# Gender recode function\n",
    "def gender(row):\n",
    "    if row == 'female_disgust' or 'female_fear' or 'female_happy' or 'female_sad' or 'female_surprise' or 'female_neutral':\n",
    "        return 'female'\n",
    "    elif row == 'male_angry' or 'male_fear' or 'male_happy' or 'male_sad' or 'male_surprise' or 'male_neutral' or 'male_disgust':\n",
    "        return 'male'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Emotion by gender accuracy  \n",
    "So lets visualise how well we have done for the Emotion by Gender model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the predictions file \n",
    "finaldf = pd.read_csv(\"Predictions.csv\")\n",
    "classes = finaldf.actualvalues.unique()\n",
    "classes.sort()    \n",
    "\n",
    "# Confusion matrix \n",
    "c = confusion_matrix(finaldf.actualvalues, finaldf.predictedvalues)\n",
    "print(accuracy_score(finaldf.actualvalues, finaldf.predictedvalues))\n",
    "print_confusion_matrix(c, class_names = classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report \n",
    "classes = finaldf.actualvalues.unique()\n",
    "classes.sort()    \n",
    "print(classification_report(finaldf.actualvalues, finaldf.predictedvalues, target_names=classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Absolute accuracy for the gender by emotions is 43%. Whilst that may not seem high at first but remember, a random guess correct is 1 out of 14 which is 7%. So 43% is huge! The heat map plot below will do justice in illustrating how good the results is. And note we have only just scratched the surface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------\n",
    "#### Gender accuracy result \n",
    "if you notice, that the gender classification is more accurate. So lets group them up and measure the accuracy again?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modidf = finaldf\n",
    "modidf['actualvalues'] = finaldf.actualvalues.replace({'female_angry':'female'\n",
    "                                       , 'female_disgust':'female'\n",
    "                                       , 'female_fear':'female'\n",
    "                                       , 'female_happy':'female'\n",
    "                                       , 'female_sad':'female'\n",
    "                                       , 'female_surprise':'female'\n",
    "                                       , 'female_neutral':'female'\n",
    "                                       , 'male_angry':'male'\n",
    "                                       , 'male_fear':'male'\n",
    "                                       , 'male_happy':'male'\n",
    "                                       , 'male_sad':'male'\n",
    "                                       , 'male_surprise':'male'\n",
    "                                       , 'male_neutral':'male'\n",
    "                                       , 'male_disgust':'male'\n",
    "                                      })\n",
    "\n",
    "modidf['predictedvalues'] = finaldf.predictedvalues.replace({'female_angry':'female'\n",
    "                                       , 'female_disgust':'female'\n",
    "                                       , 'female_fear':'female'\n",
    "                                       , 'female_happy':'female'\n",
    "                                       , 'female_sad':'female'\n",
    "                                       , 'female_surprise':'female'\n",
    "                                       , 'female_neutral':'female'\n",
    "                                       , 'male_angry':'male'\n",
    "                                       , 'male_fear':'male'\n",
    "                                       , 'male_happy':'male'\n",
    "                                       , 'male_sad':'male'\n",
    "                                       , 'male_surprise':'male'\n",
    "                                       , 'male_neutral':'male'\n",
    "                                       , 'male_disgust':'male'\n",
    "                                      })\n",
    "\n",
    "classes = modidf.actualvalues.unique()  \n",
    "classes.sort() \n",
    "\n",
    "# Confusion matrix \n",
    "c = confusion_matrix(modidf.actualvalues, modidf.predictedvalues)\n",
    "print(accuracy_score(modidf.actualvalues, modidf.predictedvalues))\n",
    "print_confusion_matrix(c, class_names = classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report \n",
    "classes = modidf.actualvalues.unique()\n",
    "classes.sort()    \n",
    "print(classification_report(modidf.actualvalues, modidf.predictedvalues, target_names=classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With just gender we get a 80% accuracy. The model is especially precise in capturing female voices. However, male voices tends to be harder and it does make higher mistakes thinking its female. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Emotion accuracy\n",
    "We'll now ignore the gender part and just super group them into the 7 core emotions. Lets see what we get..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modidf = pd.read_csv(\"Predictions.csv\")\n",
    "modidf['actualvalues'] = modidf.actualvalues.replace({'female_angry':'angry'\n",
    "                                       , 'female_disgust':'disgust'\n",
    "                                       , 'female_fear':'fear'\n",
    "                                       , 'female_happy':'happy'\n",
    "                                       , 'female_sad':'sad'\n",
    "                                       , 'female_surprise':'surprise'\n",
    "                                       , 'female_neutral':'neutral'\n",
    "                                       , 'male_angry':'angry'\n",
    "                                       , 'male_fear':'fear'\n",
    "                                       , 'male_happy':'happy'\n",
    "                                       , 'male_sad':'sad'\n",
    "                                       , 'male_surprise':'surprise'\n",
    "                                       , 'male_neutral':'neutral'\n",
    "                                       , 'male_disgust':'disgust'\n",
    "                                      })\n",
    "\n",
    "modidf['predictedvalues'] = modidf.predictedvalues.replace({'female_angry':'angry'\n",
    "                                       , 'female_disgust':'disgust'\n",
    "                                       , 'female_fear':'fear'\n",
    "                                       , 'female_happy':'happy'\n",
    "                                       , 'female_sad':'sad'\n",
    "                                       , 'female_surprise':'surprise'\n",
    "                                       , 'female_neutral':'neutral'\n",
    "                                       , 'male_angry':'angry'\n",
    "                                       , 'male_fear':'fear'\n",
    "                                       , 'male_happy':'happy'\n",
    "                                       , 'male_sad':'sad'\n",
    "                                       , 'male_surprise':'surprise'\n",
    "                                       , 'male_neutral':'neutral'\n",
    "                                       , 'male_disgust':'disgust'\n",
    "                                      })\n",
    "\n",
    "classes = modidf.actualvalues.unique() \n",
    "classes.sort() \n",
    "\n",
    "# Confusion matrix \n",
    "c = confusion_matrix(modidf.actualvalues, modidf.predictedvalues)\n",
    "print(accuracy_score(modidf.actualvalues, modidf.predictedvalues))\n",
    "print_confusion_matrix(c, class_names = classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report \n",
    "classes = modidf.actualvalues.unique()\n",
    "classes.sort()    \n",
    "print(classification_report(modidf.actualvalues, modidf.predictedvalues, target_names=classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "50%, not too shabby indeed. The precision and recall for 'Surprise' and 'Angry' is pretty good in particular "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"final\"></a>\n",
    "## 5. Final thoughts \n",
    "The gender seperation turns out to be a curcial implementation in order to accurately classify emotions. Upon closer inspection of the confusion matrix, it seems that female tends to express emotions in a more, obvious manner, for the lack of a better word. Whilst males tend to be very placid or subtle. This is probably why we see the error rate amongst males are really high. For example, male happy and angry gets mixed up quite often. \n",
    "\n",
    "In our next section we will be checking for generalisability of this initial baseline solution before  before implementing further enhancements, followed by an audio streamer that will give us the capability of predicting the emotions of a segment of the audio call.  \n",
    "\n",
    "This section of the notebook borrowed heavily from this [repository](https://github.com/MITESHPUTHRANNEU/Speech-Emotion-Analyzer). The original author may have overstated the accuracy as I wasn't able to replicate the accuracy results but, by in large the approach is pretty sound and I've taken his work as a blueprint to setup my own here. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
